{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "patient-hunter",
   "metadata": {},
   "source": [
    "# Advanced Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-arizona",
   "metadata": {},
   "source": [
    "Now that I have found a base model to work with, let's see how I can improve upon or build a better competing model. In this part of the project, I will be attempting to optimize the Ridge Regression base model. In addition, I will build RandomForest, Light Gradient Boosting Machine (LGBM), and Extreme Gradient Boosting (XGBoost) models and compare their performance against the optimized Ridge Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lyric-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finite-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../capstone2-housing/documents/final_housing_df.csv', index_col=0)\n",
    "X_train = pickle.load(open('X_train', 'rb'))\n",
    "X_test = pickle.load(open('X_test', 'rb'))\n",
    "y_train = pickle.load(open('y_train', 'rb'))\n",
    "y_test = pickle.load(open('y_test', 'rb'))\n",
    "\n",
    "model1 = pickle.load(open('RR_base', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-philip",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-faith",
   "metadata": {},
   "source": [
    "Okay, now that everything has been imported, the fun can begin. First, I want to try to improve on my Ridge Regression base model. In my base modelling, I identified the top three positive and top three negative importance features, let's retrace the steps and get a list of the top 10 coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-friendly",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complicated-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model1.coef_\n",
    "feature_dict = {}\n",
    "for coef, feat in zip(coefs, X_train.columns):\n",
    "    feature_dict[round(coef)] = feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "composite-catering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145606, 124965, 92548, 72769, 66604, 62588, 51464, 47327, 43907, 43710]\n",
      "[-404345, -170360, -110046, -102164, -58605, -56044, -52028, -50921, -47994, -40665]\n"
     ]
    }
   ],
   "source": [
    "positive_coefs = sorted([round(coef) for coef in coefs if coef >=0], reverse=True)\n",
    "negative_coefs = sorted([round(coef) for coef in coefs if coef < 0], reverse=True, key=abs)\n",
    "top_pos_feat = positive_coefs[:10]\n",
    "top_neg_feat = negative_coefs[:10]\n",
    "\n",
    "print(top_pos_feat)\n",
    "print(top_neg_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "assumed-association",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fence_GdPrv : 145606\n",
      "Exterior1st_AsbShng : 124965\n",
      "RoofMatl_Metal : 92548\n",
      "YearBuilt_1934 : 72769\n",
      "PoolQC_Fa : 66604\n",
      "RoofMatl_Membran : 62588\n",
      "RoofMatl_ClyTile : 51464\n",
      "OverallCond_1 : 47327\n",
      "RoofMatl_WdShngl : 43907\n",
      "RoofStyle_Flat : 43710\n",
      "RoofMatl_CompShg : -404345\n",
      "Condition2_RRAe : -170360\n",
      "PoolQC_Na : -110046\n",
      "PoolQC_Gd : -102164\n",
      "YearBuilt_1893 : -58605\n",
      "GarageYrBlt_1906.0 : -56044\n",
      "YearBuilt_1965 : -52028\n",
      "GarageYrBlt_1933.0 : -50921\n",
      "ExterCond_TA : -47994\n",
      "GarageYrBlt_1920.0 : -40665\n"
     ]
    }
   ],
   "source": [
    "top_features = positive_coefs[:10] + negative_coefs[:10]\n",
    "for i in top_features:\n",
    "    print(feature_dict.get(i), \":\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-respondent",
   "metadata": {},
   "source": [
    "As there are many way we can approach this, I will try the following three subsets for feature selection: Top 10 Positive/10 Negative, Top 5 Positive/5 Negative, Top 3 Positive/3 Negative features. I will retrain the model for each of them and compare the results to see what difference it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-tamil",
   "metadata": {},
   "source": [
    "To do so, I will create a function that extracts the top k features based on the model coefficients, trains and evaluates the model with those features, and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "explicit-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_feature_score(k):\n",
    "    selected_features = []\n",
    "    top_k = positive_coefs[:k] + negative_coefs[:k]\n",
    "    for coef in top_k:\n",
    "        selected_features.append(feature_dict.get(coef))\n",
    "    X_train_k = X_train[selected_features]\n",
    "    X_test_k = X_test[selected_features]\n",
    "    model1.fit(X_train_k, y_train)\n",
    "    mod1_y_test_pred = model1.predict(X_test_k)\n",
    "    mod1_r2_test = model1.score(X_test_k, y_test)\n",
    "    mod1_mae_test = mean_absolute_error(y_test, mod1_y_test_pred)\n",
    "    return(mod1_r2_test, mod1_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-beginning",
   "metadata": {},
   "source": [
    "Time to find the best k value. I will use my function to iterate over k values the length of the list of negative coefficients (as that list is smaller than the list of positive coefficients). I will gather te results in two separate lists: R2 scores and MAE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dense-teacher",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K with best R2 score: 190 , R2 Score: 0.8458229177969665 , MAE: 20716.53215373754\n",
      "K with smallest MAE: 200 , R2 Score: 0.8394433917505123 , MAE: 20691.08981478877\n"
     ]
    }
   ],
   "source": [
    "iterations = len(negative_coefs)\n",
    "r2s = []\n",
    "maes = []\n",
    "\n",
    "for num in range(1, iterations):\n",
    "    r2_score, mae_score = k_feature_score(num)\n",
    "    r2s.append(r2_score)\n",
    "    maes.append(mae_score)\n",
    "\n",
    "r2_index = r2s.index(max(r2s))\n",
    "mae_index = maes.index(min(maes))\n",
    "print('K with best R2 score:', r2_index+1, ', R2 Score:', max(r2s), ', MAE:', maes[r2_index])\n",
    "print('K with smallest MAE:', mae_index+1, ', R2 Score:', r2s[mae_index], ', MAE:', min(maes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-louisiana",
   "metadata": {},
   "source": [
    "Based on the very low difference in MAE, the model performs best when for the top 190 features. I will train the model and summarize the scores below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "running-campbell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R2 score: 0.8458229177969665 , Ridge Regression MAE: 20716.53215373754\n"
     ]
    }
   ],
   "source": [
    "r2_score, mae_score = k_feature_score(190)\n",
    "print('Ridge Regression R2 score:', r2_score, ', Ridge Regression MAE:', mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-appreciation",
   "metadata": {},
   "source": [
    "##### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-establishment",
   "metadata": {},
   "source": [
    "Now that I have fine tuned the Ridge Regrssion, let's see if there are other models that could perform better. The first one I want to try is Random Forest Regression. I will start with establishing a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "developing-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(random_state = 123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-accommodation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-thomson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "republican-hybrid",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
