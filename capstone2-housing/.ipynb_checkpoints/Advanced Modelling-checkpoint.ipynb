{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nutritional-helena",
   "metadata": {},
   "source": [
    "# Advanced Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-activity",
   "metadata": {},
   "source": [
    "Now that I have found a base model to work with, let's see how I can improve upon or build a better competing model. In this part of the project, I will be attempting to optimize the Ridge Regression base model. In addition, I will build RandomForest and Light Gradient Boosting Machine (LGBM) models and compare their performance against the optimized Ridge Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excessive-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "billion-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../capstone2-housing/documents/final_housing_df.csv', index_col=0)\n",
    "X_train = pickle.load(open('X_train', 'rb'))\n",
    "X_test = pickle.load(open('X_test', 'rb'))\n",
    "y_train = pickle.load(open('y_train', 'rb'))\n",
    "y_test = pickle.load(open('y_test', 'rb'))\n",
    "\n",
    "model1 = pickle.load(open('RR_base', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-bridges",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-cooler",
   "metadata": {},
   "source": [
    "Okay, now that everything has been imported, the fun can begin. First, I want to try to improve on my Ridge Regression base model. In my base modelling, I finetuned the hyperparameter alpha=0.1. I also identified the top three positive and top three negative importance features, let's retrace the steps and get a list of the top 10 coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-canada",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "figured-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model1.coef_\n",
    "feature_dict = {}\n",
    "for coef, feat in zip(coefs, X_train.columns):\n",
    "    feature_dict[round(coef)] = feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "induced-berry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145606, 124965, 92548, 72769, 66604, 62588, 51464, 47327, 43907, 43710]\n",
      "[-404345, -170360, -110046, -102164, -58605, -56044, -52028, -50921, -47994, -40665]\n"
     ]
    }
   ],
   "source": [
    "positive_coefs = sorted([round(coef) for coef in coefs if coef >=0], reverse=True)\n",
    "negative_coefs = sorted([round(coef) for coef in coefs if coef < 0], reverse=True, key=abs)\n",
    "top_pos_feat = positive_coefs[:10]\n",
    "top_neg_feat = negative_coefs[:10]\n",
    "\n",
    "print(top_pos_feat)\n",
    "print(top_neg_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "primary-whale",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fence_GdPrv : 145606\n",
      "Exterior1st_AsbShng : 124965\n",
      "RoofMatl_Metal : 92548\n",
      "YearBuilt_1934 : 72769\n",
      "PoolQC_Fa : 66604\n",
      "RoofMatl_Membran : 62588\n",
      "RoofMatl_ClyTile : 51464\n",
      "OverallCond_1 : 47327\n",
      "RoofMatl_WdShngl : 43907\n",
      "RoofStyle_Flat : 43710\n",
      "RoofMatl_CompShg : -404345\n",
      "Condition2_RRAe : -170360\n",
      "PoolQC_Na : -110046\n",
      "PoolQC_Gd : -102164\n",
      "YearBuilt_1893 : -58605\n",
      "GarageYrBlt_1906.0 : -56044\n",
      "YearBuilt_1965 : -52028\n",
      "GarageYrBlt_1933.0 : -50921\n",
      "ExterCond_TA : -47994\n",
      "GarageYrBlt_1920.0 : -40665\n"
     ]
    }
   ],
   "source": [
    "top_features = positive_coefs[:10] + negative_coefs[:10]\n",
    "for i in top_features:\n",
    "    print(feature_dict.get(i), \":\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-least",
   "metadata": {},
   "source": [
    "There are different ways I can approach this. For this project, I have decided to loop over a range of numbers (k) that will create a subset of the top k positive and top k negative features. I will use each subset to retrain the model with a cross-validation function and retreive the MAE and MAPE scores for each of the subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-familiar",
   "metadata": {},
   "source": [
    "First, I will rebuild a cross-validation function (built in Advanced Logistic Regression case-study for this course). That returns MAE and MAPE scores for each iteration.  \n",
    "Next, I will create a function that extracts the top k features based on the model coefficients, uses the cross-validation on those features, and returns the MAE and MAPE results for each value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "false-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score(clf, x, y, score_func=mean_absolute_error):\n",
    "    #print(str(type(x)) + ' ' + str(x.shape) + ' ' + str(type(y)) + ' ' + str(y.shape))\n",
    "    result_default = 0\n",
    "    result_mape = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        y_pred = clf.predict(x[test])\n",
    "        result_default += score_func(y_pred, y[test]) # evaluate score function on held-out data\n",
    "        result_mape += np.mean(np.abs((y[test] - y_pred)/y[test]))\n",
    "    return (result_default / nfold), (result_mape / nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "juvenile-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a number as an input, creates a mask from two global lists (positive_coefs, negative_coefs), \n",
    "# creates a new X_train with that mask, performs cross-validation, and returns a tuple \n",
    "\n",
    "def k_feature_score(k):\n",
    "    selected_features = []\n",
    "    top_k = positive_coefs[:k] + negative_coefs[:k]\n",
    "    for coef in top_k:\n",
    "        selected_features.append(feature_dict.get(coef))\n",
    "    X_train_k = X_train[selected_features]\n",
    "    X_train_k = X_train_k.to_numpy()\n",
    "    y_train_k = y_train.to_numpy()\n",
    "    mae_scores, mape_scores = cv_score(model1, X_train_k, y_train_k)\n",
    "    return (mae_scores, mape_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-typing",
   "metadata": {},
   "source": [
    "Time to find the best k value. I will use my function to iterate over k values the length of the list of negative coefficients (as that list is smaller than the list of positive coefficients). I will gather the results in one list of k_value_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competitive-hello",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(58071.7187427021, 0.36899258314607397), (58256.05498179355, 0.3683679072485024), (58277.21074712209, 0.36826808582250353), (58616.89078924405, 0.3695768130229524), (58399.37033889587, 0.3685560321865939), (58386.7862944083, 0.36855202894447625), (58551.96675143474, 0.36988967697163266), (55444.54528770454, 0.35618164459451684), (55345.44107293371, 0.3557203997967383), (55497.44291198782, 0.35582222548128184), (55513.87637112511, 0.35609932101104796), (55587.28634021359, 0.3568382639304705), (55636.37578508242, 0.35715486069836766), (55603.25741043497, 0.3566394988007596), (55529.98086508851, 0.3565144948436224), (51767.541379030634, 0.326393356380941), (51818.684610712124, 0.3263932008375601), (51987.29666734776, 0.32701809728497994), (50913.659259072614, 0.32004135489552804), (50504.75119438496, 0.31300494759027525), (50400.109309545325, 0.3125092878808913), (46745.23433217629, 0.2929675040063444), (46785.84113853068, 0.29313903264053837), (46655.42862860513, 0.2919053271905574), (46721.09425162983, 0.2923400136531636), (46735.79360866215, 0.2924305258935989), (46682.39787143219, 0.2922192070338109), (45007.66938994444, 0.28347977666940183), (44933.0108479685, 0.2819175193148181), (44985.89028279575, 0.28210983536216006), (44470.43424639451, 0.27707632570406515), (44335.33497457368, 0.2765879875506282), (43979.93685109809, 0.27349722470143145), (43555.48713679022, 0.27129255414769327), (43231.012228922846, 0.26876099681631194), (43122.41555047939, 0.26993217759969795), (41350.806966017255, 0.25873224500870573), (41341.7677119685, 0.258423476490765), (41434.63393945822, 0.2590676688529761), (40558.64788624429, 0.2523476448019165), (40198.05597152865, 0.2504019526377677), (40157.441898698606, 0.2502031230027338), (40149.910783637715, 0.25034315012425506), (40174.37312239781, 0.2504210503434788), (40221.9622715746, 0.2508364753978867), (40086.08487383055, 0.2498781145890284), (40220.75019583527, 0.25029256010743), (40305.785278516334, 0.25093668253695034), (39905.01457246026, 0.248032393852112), (39541.49984246304, 0.24567606519429255), (38669.30900002633, 0.24449226601250862), (38693.98145665279, 0.24483051107245105), (38734.09199111749, 0.24527689381556997), (38775.21958789965, 0.24559394636932877), (38862.94809562316, 0.24625669182394955), (38981.30264129083, 0.2463374973604353), (38823.33144286566, 0.2448523021223758), (38792.68875512366, 0.24455501730120915), (38756.374019554394, 0.24430512705028873), (38805.613873148846, 0.24436757565894052), (38767.786340373736, 0.2439247530989686), (38548.724554149514, 0.2420579766707077), (38292.35101962862, 0.2401973876050699), (38393.37663702595, 0.24115218240778788), (38607.5458432603, 0.2421008750975584), (38362.61566060927, 0.2408935602125136), (38475.147187803836, 0.24165056433033222), (28454.51216114632, 0.17368088121925998), (26881.78524436428, 0.164363662286594), (26927.462591422147, 0.16457031058519372), (26237.559038496373, 0.16174304383915292), (26380.975663830854, 0.1637754786525992), (25711.69925624312, 0.15825653012385654), (25350.106657507407, 0.155045289130406), (25250.592752417782, 0.154002259699772), (25236.40353793736, 0.15391302015033476), (25365.703463707974, 0.15500058318872675), (25323.091565679373, 0.15464451871243312), (25039.47216521894, 0.1528320338920526), (24993.727623102408, 0.1527310737854472), (25050.720761790668, 0.15335399526085117), (24207.23547802721, 0.14706589345143312), (24252.60539330669, 0.14728091417165362), (24231.482718698902, 0.14698845388515322), (24348.578750007255, 0.14786940692839984), (23987.709443622898, 0.1446087062506206), (23903.86131240887, 0.14334625365875076), (23905.152402308046, 0.14364652550544343), (23921.678696023806, 0.14378622421260084), (24111.844434408973, 0.14461452821405385), (24089.85656566994, 0.144714916002687), (23983.256303344166, 0.1442265524988811), (24026.21691713753, 0.1446168878474242), (23772.039962686853, 0.14361757016939147), (23768.636781473768, 0.14338123226470528), (23845.480601968546, 0.1441609822820255), (23925.19917626118, 0.1447898283509393), (24030.3451257781, 0.14457786404655942), (24050.43210653236, 0.1445997411393875), (24075.31106106619, 0.14475850952915867), (23781.039433072663, 0.14224485564598102), (23502.53718551559, 0.13978696188955425), (23526.268710246975, 0.1400096969865708), (23492.20096947137, 0.13963073878625226), (23507.306473009485, 0.1395386388219387), (23500.581339760865, 0.1397360319703505), (23537.74834661833, 0.14010397485297355), (23581.837521915066, 0.1405120335355928), (23615.93837136984, 0.14073854589045212), (23486.381901461344, 0.13982000827024813), (23327.235241020346, 0.13733567089935955), (23360.194569106105, 0.13760899366678386), (23356.71674885588, 0.13735594641194698), (23227.535313973636, 0.13704605318826615), (23175.856449541843, 0.13598508132950227), (23299.45975502443, 0.13773568126333785), (23396.685833211424, 0.13844819946221235), (23412.428831665522, 0.13846145220322056), (23409.726627717617, 0.13848103724222993), (23379.547466505235, 0.13845132071302643), (23377.267238863016, 0.13829859337713107), (23351.912414966286, 0.13815034012205693), (23301.171045454765, 0.13753723810118051), (23355.676533554164, 0.13799393416196723), (23392.04633774777, 0.1382073924520581), (23451.481415682254, 0.13884826093008812), (23472.470735100727, 0.13885872008649147), (23488.24944978882, 0.13890928850804718), (23458.029832631502, 0.13901489858533717), (23220.65505142911, 0.13760950143132705), (22226.602547539303, 0.13150863871666046), (22115.990037697855, 0.13109621314496311), (22111.10311834792, 0.13052461151079991), (21902.91291461408, 0.12929697042749383), (21908.67576203137, 0.1293780041612976), (21895.19633747243, 0.1292695238061466), (21964.479350862974, 0.12985385316535653), (21970.480312756095, 0.12984545534595537), (21942.796948781765, 0.12972134075666292), (21963.56400873761, 0.12988250469984536), (22016.794692382253, 0.13030986905402067), (21993.796366686547, 0.1302873275923579), (20817.01849296917, 0.12362469420299238), (20854.247002666434, 0.12392708825165437), (20962.980737210157, 0.12466799760488304), (20990.5388502, 0.12497160556654266), (21037.386707045283, 0.12526436618792142), (20997.524398420195, 0.12525258040217885), (21152.760762276786, 0.12680481026723062), (21298.441785296836, 0.12754794215632922), (21365.757774659556, 0.12817585159257638), (21378.184851637045, 0.1287179510819943), (21407.06635081406, 0.12887217063359807), (21441.21820437754, 0.12887378020929932), (21398.419941767654, 0.12882143058654955), (21412.045034405, 0.12884193755252324), (21392.040874856324, 0.12868010247331962), (21493.7289055641, 0.1293632105153038), (21300.22480781548, 0.12831527025719267), (21359.32500335382, 0.12871675523568474), (21440.525319912984, 0.1292460143728218), (21530.196611554806, 0.12968259021525166), (21588.035060706483, 0.1300407592791833), (21537.94014408494, 0.12949822481934653), (21573.186875341526, 0.12974745928128656), (21592.453986659915, 0.1297385115316982), (21678.231843286692, 0.13059062189079468), (21730.57713698373, 0.13125480016804894), (21763.85726434192, 0.13135378895067415), (21824.542411162052, 0.1316258524569188), (21839.306665577216, 0.13174460017345288), (21935.706607803124, 0.13253546622790183), (21934.37296487618, 0.1323350538215659), (21929.156601039656, 0.1320267629182618), (21986.280920051944, 0.13234695426785006), (21937.166616635866, 0.13207690900731903), (22016.73924902691, 0.13255478419657465), (22052.154420242703, 0.13304063940073707), (22084.96529536102, 0.13362899100602704), (21886.1196121399, 0.132185656485224), (21891.888413104654, 0.13229622910919892), (21634.739230930427, 0.13023546498275407), (21611.389146047342, 0.13023200381362965), (21621.407259101255, 0.13033399881041402), (21594.05072322095, 0.13004644876322988), (21609.559484676767, 0.13020326566820678), (21633.323805156, 0.13031223190418548), (21626.95781066518, 0.1303202653677072), (21649.632173081387, 0.13036631040817945), (21346.03511372063, 0.12941959448677168), (21502.71765334334, 0.13076431027020122), (21517.198739353036, 0.13079794654166516), (21593.36935935191, 0.13130376047529752), (21368.75648248276, 0.13027232838753386), (21451.07209118989, 0.13090289040522443), (21424.588390138328, 0.1307450122772736), (21441.89883287139, 0.1310335143146848), (21539.9686979175, 0.1318401509358082), (21611.214890873693, 0.1323909756457072), (21634.004360214578, 0.13267066115767262), (21653.674614207823, 0.1327492432668202), (21664.266205046715, 0.13283774025793302), (21710.684684444073, 0.1332616746822933), (21700.511684558653, 0.13318067314238036), (21837.770350913248, 0.1340743324781674), (21832.352170145874, 0.1341187787755619), (21916.947182493597, 0.13467886152518696), (21925.703071280972, 0.13472664237617896), (21937.224337580792, 0.1347967050700167), (21928.003103870917, 0.1348294485743891), (21914.804016474765, 0.13474858069094486), (21918.910439879015, 0.13482486666371649), (21913.248245490828, 0.1347349334260019), (21949.441470817932, 0.13504625384257668), (21953.302718600848, 0.13509391632997123), (22021.760411379782, 0.13569291449583695), (22185.396487826034, 0.1368948824823111), (22223.79916649072, 0.13726113893111158), (22307.1001887648, 0.13809795312356696), (22340.10959281303, 0.13824962225814755), (22256.881818425463, 0.1378169835375091), (22262.236959187307, 0.13785522564075697), (22295.949284744456, 0.1381445448000711), (22283.401295559182, 0.138043394824518), (22308.355969786582, 0.13829985295763797), (22410.639906276298, 0.13899104783248054), (22475.18727714112, 0.13868750995062934), (22473.691473289957, 0.13872780043329438), (22529.90734118234, 0.13907843225712496), (22621.47436977936, 0.13964642180709141), (22659.500724476635, 0.1398966869053731), (22760.886664484675, 0.14087231535818692), (22827.8519838505, 0.14146674909027981), (22912.843709150744, 0.142233158091276), (22902.188149932546, 0.1421407297594826), (22928.217430312143, 0.14225562326039182), (22993.025159932688, 0.14281124958480235), (23005.0821480047, 0.14293851619899403), (23041.552612568903, 0.14314520714236262), (23050.828542062394, 0.14313425990031153), (23136.710267656825, 0.14371314553939873), (23166.686201834975, 0.143928464769642), (23197.23005703803, 0.1440889983086629), (23194.158923741616, 0.1441108062256959), (23177.886346343406, 0.14378847688099533), (23241.39077019433, 0.1441849963016746), (23333.329003977204, 0.14527574680387306), (23383.176458025984, 0.1455330914003374), (23435.19287404062, 0.14581568490590102), (23532.92314777975, 0.14659167829561842), (23619.375725129463, 0.14730290555207848), (23669.900801968215, 0.14776672624692397), (23645.460775480908, 0.14765750257831625), (23693.989698975864, 0.14792714428786857), (23624.12196487086, 0.14727606110726432), (23654.47672278522, 0.14738577140043302), (23659.621741716153, 0.14733454703584337), (23758.172259199724, 0.1480898912435085), (23864.81038882787, 0.1488774577964943), (23744.786521760452, 0.14841635614812929), (23865.302530907364, 0.1490862334597829), (23882.83200134908, 0.14924300553547643), (24051.270362903437, 0.15052415956810017), (24050.067988949995, 0.1503722200868684), (24141.76802461422, 0.1509052353538526), (24185.266721623753, 0.1513641770994542), (24234.622447612554, 0.1515819104874377), (24289.93727445188, 0.15218036867770868), (24356.930830323232, 0.15251536971023896), (24358.369973311765, 0.15252647866929192), (24390.673936443356, 0.15295363027478867), (24429.65420231787, 0.153400136058806), (24479.758679988172, 0.15374552620224874), (24556.504647708636, 0.15449041623628548), (24632.180107067634, 0.15497951691186707), (24676.398227044992, 0.15546494119281987), (24721.340399427434, 0.15555915125445421), (24883.918832637668, 0.15677955992825204), (24984.96876943732, 0.15784128735024658), (24962.150435607975, 0.15785640329292736), (24964.640525133844, 0.15787487591817634), (25160.202227979124, 0.15950007892433338), (25265.26443613434, 0.16024956879183544), (25326.07966839367, 0.16083699711737404), (25357.54170503547, 0.16100297432856125), (25398.809593472055, 0.1612755415681698)]\n"
     ]
    }
   ],
   "source": [
    "iterations = len(negative_coefs)\n",
    "k_value_scores = []\n",
    "\n",
    "for num in range(1, iterations):\n",
    "    cv_scores = k_feature_score(num)\n",
    "    k_value_scores.append(cv_scores)\n",
    "print(k_value_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-toronto",
   "metadata": {},
   "source": [
    "Let's find the smallest score and it's corresponding k value (index+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demonstrated-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20817.01849296917, 0.12362469420299238) 143\n"
     ]
    }
   ],
   "source": [
    "best_score = min(k_value_scores)\n",
    "best_k = k_value_scores.index(best_score) + 1\n",
    "print(best_score, best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-brown",
   "metadata": {},
   "source": [
    "Based on the MAE (20817.0185) and MAPE (0.1236) scores, the model performs best when for the top 143 features. Below, I will create the mask to select the top 143 positie and negative features, train the model with those features and summarize the scores below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "further-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = []\n",
    "top_k = positive_coefs[:143] + negative_coefs[:143]\n",
    "for coef in top_k:\n",
    "    selected_features.append(feature_dict.get(coef))\n",
    "X_train_rr = X_train[selected_features]\n",
    "X_test_rr = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "graduate-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge R2 score: 0.842780751377597 , Ridge MAE: 21112.992758626242 Ridge MAPE: 0.12425707535528599\n"
     ]
    }
   ],
   "source": [
    "model1.fit(X_train_rr, y_train)\n",
    "mod1_y_test_pred = model1.predict(X_test_rr)\n",
    "mod1_r2_test = model1.score(X_test_rr, y_test)\n",
    "mod1_mae_test = mean_absolute_error(y_test, mod1_y_test_pred)\n",
    "mod1_mape_test = np.mean(np.abs((y_test - mod1_y_test_pred)/y_test))\n",
    "print('Ridge R2 score:', mod1_r2_test, ', Ridge MAE:', mod1_mae_test, 'Ridge MAPE:', mod1_mape_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-bouquet",
   "metadata": {},
   "source": [
    "| Model | R2 Score | MAE | MAPE |  \n",
    "| --- | --- | --- | --- |  \n",
    "| Baseline RR | 0.8353| 20668.8593 |  \n",
    "|Feature Selection RR | 0.8428 | 21112.9928 | 0.1242 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-extent",
   "metadata": {},
   "source": [
    "Above, I have completed feature selection for my Ridge Regression model. I have found that a subset of the top 143 positive and top 143 negative features scores the lowest MAPE. Compared to the baseline Ridge Regression model, using this subset of features has a better R2 score but a slightly worse MAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-sculpture",
   "metadata": {},
   "source": [
    "##### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-career",
   "metadata": {},
   "source": [
    "Now that I have fine tuned the Ridge Regression, let's see if there are other models that could perform better. The first one I want to try is Random Forest Regression. I will start with establishing a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifty-optics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.9778821466705006 , Random Forest 2 MAE: 6956.267291585127\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_train, ', Random Forest 2 MAE:', rfr_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "noble-thinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.8525919728615938 , Random Forest 2 MAE: 18405.52422700587\n"
     ]
    }
   ],
   "source": [
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_test, ', Random Forest 2 MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-elevation",
   "metadata": {},
   "source": [
    "Looks like the Random Forest Regressor is overfitted and needs some refining. Let's start with hyperparameter tuning. I've been using this guide to decide on which parameters to tune: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html#for-better-accuracy  \n",
    "\n",
    "I will do a grid search cross validation to see which n_estimators and min_leaf_sample score the lowest MAE. After a preliminary run through, I have decided to start the iterations at n_estimators=50, going in 10 estimator increments up to 140 estimators (anything below 50 and above 150 yielded clearly poorer results). For min_samples_leaf, I have decided to start the iterations at 5, going in 5 samples increments up to 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "outer-testing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'min_samples_leaf': 5, 'n_estimators': 140},\n",
       " -20337.170668500148,\n",
       " RandomForestRegressor(min_samples_leaf=5, n_estimators=140, n_jobs=-1,\n",
       "                       oob_score=True, random_state=123))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(oob_score=True, n_jobs=-1, random_state=123)\n",
    "params = {'n_estimators':range(50, 150, 10), 'min_samples_leaf':range(5, 65, 10)}\n",
    "grid_model = GridSearchCV(rfr, param_grid=params, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_model.fit(X_train, y_train)\n",
    "grid_model.best_params_, grid_model.best_score_, grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-bridal",
   "metadata": {},
   "source": [
    "Looks lilke using a combination of min_samples_leaf=5 and n_estimators=140 results in the lowest MAE. Let's build the model below and see how it performs in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "danish-occasions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.826766542901784 , MAE: 19127.3625817715 MAPE: 0.10956888426231229\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=140, min_samples_leaf=5, oob_score=True, n_jobs=-1, random_state=123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "rfr_mape_test = np.mean(np.abs((y_test - rfr_y_test_pred)/y_test))\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test, 'MAPE:', rfr_mape_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-stroke",
   "metadata": {},
   "source": [
    "Above, I have fine tuned the parameters for the Random Forest Regressor. We have a test R2 score of 0.8268 and MAE of 19127.3626. Slightly better MAE than my Ridge Regression model. Would feature selection be able to improve the performance even more?  \n",
    "For feature selection, I will be using the method SelectFromModel (with the parameters discussed in this article: https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f) to do my feature selection.  \n",
    "  \n",
    "First, I want to check out the feature importances from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "chronic-variation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.29897980e-03, 9.62941025e-03, 1.21303560e-02, 4.81258352e-02,\n",
       "       1.87703803e-04, 4.52227985e-03, 6.20263521e-02, 4.00043053e-02,\n",
       "       6.42809955e-03, 0.00000000e+00, 1.87021584e-01, 5.65478091e-04,\n",
       "       4.49387939e-06, 2.18701413e-02, 1.51033205e-03, 2.77478558e-03,\n",
       "       2.01419954e-03, 1.53801809e-02, 3.41817463e-01, 1.93207498e-02,\n",
       "       4.33383090e-03, 3.76978492e-03, 3.23994758e-04, 0.00000000e+00,\n",
       "       3.02122934e-04, 0.00000000e+00, 0.00000000e+00, 1.14938496e-03,\n",
       "       1.49268065e-03, 4.87228838e-04, 8.27075705e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.54364592e-04, 3.95247577e-03, 7.43085936e-05,\n",
       "       0.00000000e+00, 3.75511465e-05, 0.00000000e+00, 1.90652557e-05,\n",
       "       5.45982243e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.77412298e-06, 0.00000000e+00, 8.80642770e-04,\n",
       "       7.79513969e-04, 0.00000000e+00, 0.00000000e+00, 1.11972824e-05,\n",
       "       9.95709584e-06, 3.76782724e-06, 4.02184711e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 5.98387876e-04, 1.56072641e-05, 4.05979730e-04,\n",
       "       0.00000000e+00, 2.85130240e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.38895060e-04, 2.37192992e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       5.12466370e-04, 8.32264410e-05, 2.22323540e-05, 0.00000000e+00,\n",
       "       1.17859666e-06, 0.00000000e+00, 2.52472564e-06, 2.77395176e-05,\n",
       "       2.33959085e-05, 9.43969452e-05, 6.30200995e-04, 5.94853565e-05,\n",
       "       4.36877201e-06, 6.69758132e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.52528857e-04, 0.00000000e+00, 1.58908528e-04, 7.17090533e-06,\n",
       "       8.85604974e-04, 4.02307841e-04, 1.15318166e-05, 1.67939569e-05,\n",
       "       0.00000000e+00, 1.15117606e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.23163994e-05, 1.80503713e-05, 4.78985724e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 7.55352424e-04, 0.00000000e+00,\n",
       "       8.17244503e-05, 2.51718206e-07, 5.82634327e-06, 2.16680255e-04,\n",
       "       0.00000000e+00, 1.47433293e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.45366709e-04, 1.80749331e-06, 1.63132774e-06, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.74795274e-06, 1.03867937e-03, 7.92879274e-04,\n",
       "       7.90599349e-04, 4.26996068e-03, 9.06284938e-03, 1.15111775e-03,\n",
       "       8.34872197e-04, 0.00000000e+00, 0.00000000e+00, 3.74800473e-05,\n",
       "       2.02873298e-04, 3.96451670e-04, 1.35242299e-04, 1.77269067e-04,\n",
       "       4.83801874e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.59005207e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.55032958e-06,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.10638453e-05, 3.61685746e-05,\n",
       "       0.00000000e+00, 5.50574442e-05, 2.64879569e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.87078315e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.76489218e-07,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.02152783e-06,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.44130077e-06, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.15653727e-06, 0.00000000e+00, 5.33360272e-06, 8.09107011e-06,\n",
       "       3.38420134e-06, 1.33291181e-05, 1.98489856e-05, 2.41066768e-05,\n",
       "       6.28368750e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.94106436e-04, 0.00000000e+00, 1.25048403e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.66824078e-04, 0.00000000e+00, 0.00000000e+00, 2.99793456e-05,\n",
       "       0.00000000e+00, 1.59172565e-04, 1.46339513e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.47930687e-04, 7.78741510e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.19323754e-07, 5.00071770e-05, 0.00000000e+00,\n",
       "       1.26472061e-04, 0.00000000e+00, 1.45351636e-04, 0.00000000e+00,\n",
       "       2.82812835e-05, 8.23613282e-04, 1.17205962e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.43785094e-04, 0.00000000e+00, 1.82277952e-04,\n",
       "       2.46680490e-04, 7.57024090e-03, 4.15771972e-04, 5.25848388e-03,\n",
       "       8.94821798e-02, 0.00000000e+00, 5.58716325e-05, 3.64804965e-05,\n",
       "       0.00000000e+00, 1.51943758e-05, 1.92981869e-04, 4.40188203e-04,\n",
       "       1.84142161e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.00731789e-02, 1.87279874e-05, 4.09949496e-03, 0.00000000e+00,\n",
       "       4.42676688e-03, 2.12156932e-05, 0.00000000e+00, 3.30927160e-05,\n",
       "       0.00000000e+00, 1.07991907e-04, 1.53163784e-04, 7.92498997e-04,\n",
       "       8.37095905e-06, 3.11773799e-05, 3.41802554e-04, 1.69521355e-04,\n",
       "       2.88553142e-05, 2.01996074e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.90428822e-05, 2.14001105e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.05635190e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.29769920e-03,\n",
       "       0.00000000e+00, 9.66312886e-05, 0.00000000e+00, 3.93029253e-04,\n",
       "       2.07863320e-03, 2.02021706e-03, 4.41552877e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.50556099e-04, 6.95419744e-03,\n",
       "       5.15732363e-05, 2.73816849e-03, 1.26699692e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.37424748e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.23693763e-04, 2.99141730e-04, 0.00000000e+00,\n",
       "       6.95017422e-04, 1.93991577e-03, 0.00000000e+00, 2.35784856e-04,\n",
       "       0.00000000e+00, 3.22573089e-03, 0.00000000e+00, 1.49241678e-04,\n",
       "       0.00000000e+00, 1.48535791e-03, 3.62950668e-06, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.90116737e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.89707242e-06, 6.19698747e-07, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.45444903e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.30300996e-05, 1.35640936e-03, 0.00000000e+00, 5.58280322e-04,\n",
       "       6.51914251e-04, 0.00000000e+00, 1.55491530e-05, 0.00000000e+00,\n",
       "       4.06856851e-06, 0.00000000e+00, 6.51270450e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.08607303e-05, 0.00000000e+00,\n",
       "       6.64350367e-04, 3.58453517e-04, 0.00000000e+00, 1.60433108e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.51260652e-06, 7.57863241e-06, 3.54425292e-05, 0.00000000e+00,\n",
       "       1.78249664e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.32095492e-06, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.84777809e-04, 0.00000000e+00, 9.62854828e-04, 8.31580243e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.90200241e-04,\n",
       "       4.28554589e-04])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-scanner",
   "metadata": {},
   "source": [
    "Looks like there are a lot of features that have 0 importance. I started with a feature selection of everything above 0, and after playing around with the different values, I found that using a feature importance threshold 0.0000001 slightly improves the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accurate-philadelphia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n",
      "Index(['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n",
      "       ...\n",
      "       'Fence_GdWo', 'Fence_MnPrv', 'Fence_MnWw', 'MiscFeature_Gar2',\n",
      "       'SaleType_CWD', 'SaleType_Oth', 'SaleCondition_Abnorml',\n",
      "       'SaleCondition_AdjLand', 'SaleCondition_Partial', 'YearSold'],\n",
      "      dtype='object', length=202)\n"
     ]
    }
   ],
   "source": [
    "sel = SelectFromModel(rfr, threshold=0.0000001)\n",
    "sel.fit(X_train, y_train)\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "print(len(selected_feat))\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-harris",
   "metadata": {},
   "source": [
    "I will use the 202 features with the highest importance for this model printed above to create new X_train/X_test sets and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reflected-first",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.8269043351422135 , MAE: 19113.826673566284 MAPE: 0.10953477994107555\n"
     ]
    }
   ],
   "source": [
    "# rfr_mape_test calculates the mean average percentage error\n",
    "X_train_rfr = X_train[selected_feat]\n",
    "X_test_rfr = X_test[selected_feat]\n",
    "\n",
    "rfr.fit(X_train_rfr, y_train)\n",
    "rfr_y_test_pred = rfr.predict(X_test_rfr)\n",
    "rfr_r2_test = rfr.score(X_test_rfr, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "rfr_mape_test = np.mean(np.abs((y_test - rfr_y_test_pred)/y_test))\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test, 'MAPE:', rfr_mape_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-toronto",
   "metadata": {},
   "source": [
    "Selecting a subset of the 20 features with the highest importance slightly improved the performance of the model. The best Random Forest Regression performance scores that I have found in this part of the project are:  \n",
    "R2: 0.7594 and MAE: 22925.2674  \n",
    "Let's add that to my comparison, before moving on to LGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-bahrain",
   "metadata": {},
   "source": [
    "| Model | R2 Score | MAE | MAPE |  \n",
    "| --- | --- | --- | --- |  \n",
    "| Baseline RR | 0.8353| 20668.8593 |  \n",
    "| Feature Selection RR | 0.8428 | 21112.9928 | 0.1242 |  \n",
    "| Baseline RFR | 0.8526 | 18405.5242 |\n",
    "| Hyperparameter RFR | 0.8268 | 19127.3626 | --- | \n",
    "| Feature Selection RFR | 0.8269 | 19113.8267 | 0.1095 |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-condition",
   "metadata": {},
   "source": [
    "##### Light Gradient Boosted Machine Algorithm (LGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-contest",
   "metadata": {},
   "source": [
    "So far, I have built a Ridge Regression model and a Random Forest model. The Ridge Regression model is currently the best performing one.  \n",
    "Finally, I want to build a LGBM model to see if that model could outperform my Ridge Regression. As before, I will begin with building a base model, training and testing it on the data as is. For this part of the project, I have been using this site as a guide: https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "complimentary-offering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN R2: 0.9776385877338646 , MAE: 5647.614989434855\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMRegressor(random_state=123)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "print('TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "round-willow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.8507357166979355 , MAE: 17327.200122753264\n"
     ]
    }
   ],
   "source": [
    "lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-melbourne",
   "metadata": {},
   "source": [
    "The train and test results show that the model is overfitted. Let's see if that can be fixed by hyperparameter tuning. I will tune the parameters num_leaves and min_child_samples (to prevent overfitting). As before, I will iterate over a range of possible values (in incfrements of ten) and find which combination performs the best based on their MAE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lyric-timothy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'min_child_samples': 30, 'num_leaves': 10},\n",
       " -18134.492048100907,\n",
       " LGBMRegressor(min_child_samples=30, num_leaves=10, random_state=123))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = LGBMRegressor(num_leaves=i, random_state=123)\n",
    "params = {'num_leaves':range(10, 160, 10), 'min_child_samples':range(10, 300, 10)}\n",
    "grid_model = GridSearchCV(lgbm, param_grid=params, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_model.fit(X_train, y_train)\n",
    "grid_model.best_params_, grid_model.best_score_, grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-mission",
   "metadata": {},
   "source": [
    "Using num_leaves=10 and min_child_samples=30 seems to be the best combination. Let's rebuild the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "impressed-thumb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN R2: 0.9480475575523535 , MAE: 11114.138353166909\n",
      "TEST R2: 0.8677917704363621 , MAE: 16973.465692653655\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMRegressor(num_leaves=10, min_child_samples=30, random_state=123)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "print('TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)\n",
    "lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-saver",
   "metadata": {},
   "source": [
    "Now that I have tuned some of the parameters, I will take a look at feature importance. I will recreate the steps I did for feature importance for my random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fitting-native",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 49, 10, 50,  0, 17, 52, 45, 23,  0, 90,  1,  0,  4,  6, 11, 10,\n",
       "       13, 27, 47, 14, 26,  1,  0,  6,  0,  0,  2,  6,  0,  4,  0,  0,  0,\n",
       "        4,  4,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  1,  0,  0,\n",
       "        0,  0,  0,  2,  1,  0,  1, 11,  4,  0,  2,  0,  0,  0,  5,  0,  0,\n",
       "        2,  0,  1,  0,  0,  0,  0,  5,  0,  0, 11,  6,  0,  0,  0,  0,  0,\n",
       "        0,  0, 13,  4,  1,  0,  0,  0, 15,  0,  0,  0,  0,  0, 11,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,\n",
       "        0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  3,  1,  1, 10, 11,  4,\n",
       "        0,  0,  0,  0,  3,  6,  1, 15,  9,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,\n",
       "        0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  0,  0,  0,\n",
       "        0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  4,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  5,  0,  0, 14,  0,\n",
       "        0,  0,  0,  5,  1,  0,  1,  0,  0,  0,  8,  0,  6,  0,  3,  3,  0,\n",
       "        0,  0,  5,  2, 11,  0,  0,  2,  4,  0,  7,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  0,  0,  0,  1,  5,\n",
       "        0,  0,  0,  0,  0,  0,  5,  0,  5,  4,  0,  0,  0,  0,  0,  0,  9,\n",
       "        0,  0,  4,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  1,  0,  0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  3,\n",
       "        6,  0,  0,  0,  3,  4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "korean-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sel = []\n",
    "feature_scores = []\n",
    "\n",
    "for i in range(1, 91):\n",
    "    lgbm = LGBMRegressor(num_leaves=10, min_child_samples=30, random_state=123)\n",
    "    sel = SelectFromModel(lgbm, threshold=i)\n",
    "    sel.fit(X_train, y_train)\n",
    "    selected_feat= X_train.columns[(sel.get_support())]\n",
    "    feature_sel.append(selected_feat)\n",
    "    X_train_lgbm = X_train[selected_feat]\n",
    "    cv_scores = cross_val_score(lgbm, X_train_lgbm, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "    feature_scores.append(np.mean(np.abs(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "challenging-cabin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MAE: 17921.964540403253 , Best number of features 72 , Best Features: Index(['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF',\n",
      "       'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath',\n",
      "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
      "       'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', '3SsnPorch',\n",
      "       'MSSubClass_20', 'MSSubClass_40', 'MSSubClass_70', 'MSSubClass_75',\n",
      "       'MSZoning_RM', 'LandContour_HLS', 'LandContour_Low', 'LotConfig_FR2',\n",
      "       'Neighborhood_ClearCr', 'Neighborhood_Edwards', 'Neighborhood_Gilbert',\n",
      "       'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_StoneBr',\n",
      "       'Condition1_PosA', 'OverallQual_5', 'OverallQual_8', 'OverallQual_9',\n",
      "       'OverallQual_10', 'OverallCond_5', 'OverallCond_6', 'OverallCond_8',\n",
      "       'OverallCond_9', 'YearBuilt_2008', 'YearRemodAdd_1951',\n",
      "       'Exterior1st_CBlock', 'Exterior2nd_HdBoard', 'ExterQual_Fa',\n",
      "       'ExterCond_Ex', 'Foundation_BrkTil', 'BsmtQual_Fa', 'BsmtQual_Na',\n",
      "       'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtExposure_Av', 'BsmtExposure_Mn',\n",
      "       'BsmtFinType1_BLQ', 'BsmtFinType1_LwQ', 'HeatingQC_Fa', 'CentralAir_Y',\n",
      "       'KitchenQual_Fa', 'KitchenQual_TA', 'Functional_Maj1', 'FireplaceQu_Ex',\n",
      "       'FireplaceQu_Na', 'GarageType_Basment', 'GarageFinish_Na',\n",
      "       'PavedDrive_N', 'SaleType_Oth', 'SaleCondition_Abnorml',\n",
      "       'SaleCondition_AdjLand', 'SaleCondition_Partial', 'YearSold'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "best_feature_scores = min(feature_scores)\n",
    "best_index = feature_scores.index(best_feature_scores)\n",
    "best_features = feature_sel[best_index]\n",
    "\n",
    "print('Best MAE:', best_feature_scores, ', Best number of features', len(best_features), ', Best Features:', best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-dollar",
   "metadata": {},
   "source": [
    "Looks like we get the lowest MAE when using the 72 features listed above. Below, I will create a mask for these features and rebuild, fit and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "overhead-winter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.8668760322854312 , MAE: 16921.672259103663 MAPE: 0.09296230869131512\n"
     ]
    }
   ],
   "source": [
    "X_train_lgbm = X_train[best_features]\n",
    "X_test_lgbm = X_test[best_features]\n",
    "\n",
    "lgbm = LGBMRegressor(num_leaves=10, min_child_samples=30, random_state=123)\n",
    "lgbm.fit(X_train_lgbm, y_train)\n",
    "lgbm_y_test_pred = lgbm.predict(X_test_lgbm)\n",
    "lgbm_r2_test = lgbm.score(X_test_lgbm, y_test)\n",
    "lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "lgbm_mape_test = np.mean(np.abs((y_test - lgbm_y_test_pred)/y_test))\n",
    "print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test, 'MAPE:', lgbm_mape_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-sandwich",
   "metadata": {},
   "source": [
    "Above, I have built an LGBM, performed a GridSearchCV hyperparameter tuning that found the best combination of num_leaves (10) and min_child_samples (30), and completed cross-validation that concluded that the 72 features (listed below cell XXXX) contribute to the best performance of the model. Let's combine those findings in my comparison table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-syntax",
   "metadata": {},
   "source": [
    "##### Model Score Comparison table:  \n",
    "  \n",
    "\n",
    "| Model | R2 Score | MAE | MAPE |  \n",
    "| --- | --- | --- | --- |  \n",
    "| Baseline RR | 0.8353| 20668.8593 |  \n",
    "| Feature Selection RR | 0.8428 | 21112.9928 | 0.1242 |  \n",
    "| Baseline RFR | 0.8526 | 18405.5242 |  \n",
    "| Hyperparameter RFR | 0.8268 | 19127.3626 |  \n",
    "| Feature Selection RFR | 0.8269 | 19113.8267 | 0.1095 |  \n",
    "| Baseline LGBM | 0.8507 | 17327.2001 |    \n",
    "| Hyperparameter LGBM | 0.8678 | 16973.4657 |  \n",
    "| Feature Selection LGBM | 0.8669 | 16921.6723 | 0.0930 |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-knowing",
   "metadata": {},
   "source": [
    "Looking at the above comparison, the LGBM model (with finetuned hyperparameters and selected features) performs better than both the Ridge Regression and Random Forest Regression models. Below, I will calculate the upper and lower bound test-residuals for this model and summarize my findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "engaged-prince",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgbm_test_residuals = y_test - lgbm_y_test_pred\n",
    "_ = plt.hist(lgbm_test_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "solar-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365 173\n",
      "173 173\n"
     ]
    }
   ],
   "source": [
    "half_k = int(len(lgbm_test_residuals)*0.95 / 2)\n",
    "print(len(lgbm_test_residuals), half_k)\n",
    "sorted_pos = sorted([resid for resid in lgbm_test_residuals if resid >= 0])\n",
    "sorted_neg = sorted([abs(resid) for resid in lgbm_test_residuals if resid < 0])\n",
    "test_residuals_pos = sorted_pos[0:half_k]\n",
    "test_residuals_neg = sorted_neg[0:half_k]\n",
    "print(len(test_residuals_pos), len(test_residuals_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "otherwise-pilot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44272.87 54397.84\n"
     ]
    }
   ],
   "source": [
    "worst_lower = max(test_residuals_pos)\n",
    "worst_upper = max(test_residuals_neg)\n",
    "print(round(worst_lower, 2), round(worst_upper, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-pantyhose",
   "metadata": {},
   "source": [
    "Above, I have calculated the 95% confidence interval of the residuals, then extracted the positive and negative residuals within the 95% confidence interval, and identified the worst lower case and worst upper case scenario of residuals. In the worst case scenarios, the prediction will be 44272.87 below or 54397.84 above the actual value.\n",
    "To correct these errors, the worst case lower bound value should be added to, and the worst case upper bound should be subtracted from the model prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-thompson",
   "metadata": {},
   "source": [
    "###### LGBM SUMMARY\n",
    "\n",
    "| Model | R2 Score | MAE | MAPE | Upper/Lower bound |  \n",
    "| ---- | ---- | ---- | ---- | ---- |  \n",
    "| LGBM | 0.8669 | 16921.6723 | 0.0930 | 54397.84/44272.87 |  \n",
    "  \n",
    "  \n",
    "Hyperparameters: num_leaves=10, min_child_samples=30   \n",
    "Number of features: 72  \n",
    "List of features: 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF',\n",
    "       'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath',\n",
    "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
    "       'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', '3SsnPorch',\n",
    "       'MSSubClass_20', 'MSSubClass_40', 'MSSubClass_70', 'MSSubClass_75',\n",
    "       'MSZoning_RM', 'LandContour_HLS', 'LandContour_Low', 'LotConfig_FR2',\n",
    "       'Neighborhood_ClearCr', 'Neighborhood_Edwards', 'Neighborhood_Gilbert',\n",
    "       'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_StoneBr',\n",
    "       'Condition1_PosA', 'OverallQual_5', 'OverallQual_8', 'OverallQual_9',\n",
    "       'OverallQual_10', 'OverallCond_5', 'OverallCond_6', 'OverallCond_8',\n",
    "       'OverallCond_9', 'YearBuilt_2008', 'YearRemodAdd_1951',\n",
    "       'Exterior1st_CBlock', 'Exterior2nd_HdBoard', 'ExterQual_Fa',\n",
    "       'ExterCond_Ex', 'Foundation_BrkTil', 'BsmtQual_Fa', 'BsmtQual_Na',\n",
    "       'BsmtCond_Fa', 'BsmtCond_Gd', 'BsmtExposure_Av', 'BsmtExposure_Mn',\n",
    "       'BsmtFinType1_BLQ', 'BsmtFinType1_LwQ', 'HeatingQC_Fa', 'CentralAir_Y',\n",
    "       'KitchenQual_Fa', 'KitchenQual_TA', 'Functional_Maj1', 'FireplaceQu_Ex',\n",
    "       'FireplaceQu_Na', 'GarageType_Basment', 'GarageFinish_Na',\n",
    "       'PavedDrive_N', 'SaleType_Oth', 'SaleCondition_Abnorml',\n",
    "       'SaleCondition_AdjLand', 'SaleCondition_Partial', 'YearSold']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-inquiry",
   "metadata": {},
   "source": [
    "##### Advanced Modelling Summary    \n",
    "  \n",
    "In this step of the project, I have fine-tuned and performed feature selection on my Ridge Regression model from the baseline modelling. I have also built, hyper parameter tuned, and performed feature selection on a Random Forest Regression model and a Light Gradient Boosting Model. I found that the LGBM performs the best when using hyperparameters num_leaves=10, min_child_samples=30, and the 72 features listed above.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
