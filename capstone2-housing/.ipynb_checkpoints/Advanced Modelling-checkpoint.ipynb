{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "leading-thousand",
   "metadata": {},
   "source": [
    "# Advanced Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-compiler",
   "metadata": {},
   "source": [
    "Now that I have found a base model to work with, let's see how I can improve upon or build a better competing model. In this part of the project, I will be attempting to optimize the Ridge Regression base model. In addition, I will build RandomForest, Light Gradient Boosting Machine (LGBM), and Extreme Gradient Boosting (XGBoost) models and compare their performance against the optimized Ridge Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "apparent-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ordinary-suspension",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../capstone2-housing/documents/final_housing_df.csv', index_col=0)\n",
    "X_train = pickle.load(open('X_train', 'rb'))\n",
    "X_test = pickle.load(open('X_test', 'rb'))\n",
    "y_train = pickle.load(open('y_train', 'rb'))\n",
    "y_test = pickle.load(open('y_test', 'rb'))\n",
    "\n",
    "model1 = pickle.load(open('RR_base', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-airplane",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-savings",
   "metadata": {},
   "source": [
    "Okay, now that everything has been imported, the fun can begin. First, I want to try to improve on my Ridge Regression base model. In my base modelling, I identified the top three positive and top three negative importance features, let's retrace the steps and get a list of the top 10 coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-january",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "therapeutic-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model1.coef_\n",
    "feature_dict = {}\n",
    "for coef, feat in zip(coefs, X_train.columns):\n",
    "    feature_dict[round(coef)] = feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hawaiian-holocaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145606, 124965, 92548, 72769, 66604, 62588, 51464, 47327, 43907, 43710]\n",
      "[-404345, -170360, -110046, -102164, -58605, -56044, -52028, -50921, -47994, -40665]\n"
     ]
    }
   ],
   "source": [
    "positive_coefs = sorted([round(coef) for coef in coefs if coef >=0], reverse=True)\n",
    "negative_coefs = sorted([round(coef) for coef in coefs if coef < 0], reverse=True, key=abs)\n",
    "top_pos_feat = positive_coefs[:10]\n",
    "top_neg_feat = negative_coefs[:10]\n",
    "\n",
    "print(top_pos_feat)\n",
    "print(top_neg_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "charged-label",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fence_GdPrv : 145606\n",
      "Exterior1st_AsbShng : 124965\n",
      "RoofMatl_Metal : 92548\n",
      "YearBuilt_1934 : 72769\n",
      "PoolQC_Fa : 66604\n",
      "RoofMatl_Membran : 62588\n",
      "RoofMatl_ClyTile : 51464\n",
      "OverallCond_1 : 47327\n",
      "RoofMatl_WdShngl : 43907\n",
      "RoofStyle_Flat : 43710\n",
      "RoofMatl_CompShg : -404345\n",
      "Condition2_RRAe : -170360\n",
      "PoolQC_Na : -110046\n",
      "PoolQC_Gd : -102164\n",
      "YearBuilt_1893 : -58605\n",
      "GarageYrBlt_1906.0 : -56044\n",
      "YearBuilt_1965 : -52028\n",
      "GarageYrBlt_1933.0 : -50921\n",
      "ExterCond_TA : -47994\n",
      "GarageYrBlt_1920.0 : -40665\n"
     ]
    }
   ],
   "source": [
    "top_features = positive_coefs[:10] + negative_coefs[:10]\n",
    "for i in top_features:\n",
    "    print(feature_dict.get(i), \":\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-research",
   "metadata": {},
   "source": [
    "As there are many way we can approach this, I will try the following three subsets for feature selection: Top 10 Positive/10 Negative, Top 5 Positive/5 Negative, Top 3 Positive/3 Negative features. I will retrain the model for each of them and compare the results to see what difference it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-tracker",
   "metadata": {},
   "source": [
    "To do so, I will create a function that extracts the top k features based on the model coefficients, trains and evaluates the model with those features, and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "weird-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_feature_score(k):\n",
    "    selected_features = []\n",
    "    top_k = positive_coefs[:k] + negative_coefs[:k]\n",
    "    for coef in top_k:\n",
    "        selected_features.append(feature_dict.get(coef))\n",
    "    X_train_k = X_train[selected_features]\n",
    "    X_test_k = X_test[selected_features]\n",
    "    model1.fit(X_train_k, y_train)\n",
    "    mod1_y_test_pred = model1.predict(X_test_k)\n",
    "    mod1_r2_test = model1.score(X_test_k, y_test)\n",
    "    mod1_mae_test = mean_absolute_error(y_test, mod1_y_test_pred)\n",
    "    return(mod1_r2_test, mod1_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-spanish",
   "metadata": {},
   "source": [
    "Time to find the best k value. I will use my function to iterate over k values the length of the list of negative coefficients (as that list is smaller than the list of positive coefficients). I will gather te results in two separate lists: R2 scores and MAE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "major-rapid",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K with best R2 score: 190 , R2 Score: 0.8458229177969665 , MAE: 20716.53215373754\n",
      "K with smallest MAE: 200 , R2 Score: 0.8394433917505123 , MAE: 20691.08981478877\n"
     ]
    }
   ],
   "source": [
    "iterations = len(negative_coefs)\n",
    "r2s = []\n",
    "maes = []\n",
    "\n",
    "for num in range(1, iterations):\n",
    "    r2_score, mae_score = k_feature_score(num)\n",
    "    r2s.append(r2_score)\n",
    "    maes.append(mae_score)\n",
    "\n",
    "r2_index = r2s.index(max(r2s))\n",
    "mae_index = maes.index(min(maes))\n",
    "print('K with best R2 score:', r2_index+1, ', R2 Score:', max(r2s), ', MAE:', maes[r2_index])\n",
    "print('K with smallest MAE:', mae_index+1, ', R2 Score:', r2s[mae_index], ', MAE:', min(maes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-stake",
   "metadata": {},
   "source": [
    "Based on the very low difference in MAE, the model performs best when for the top 190 features. I will train the model and summarize the scores below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oriental-march",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R2 score: 0.8458229177969665 , Ridge Regression MAE: 20716.53215373754\n"
     ]
    }
   ],
   "source": [
    "r2_score, mae_score = k_feature_score(190)\n",
    "print('Ridge Regression R2 score:', r2_score, ', Ridge Regression MAE:', mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-board",
   "metadata": {},
   "source": [
    "##### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-therapist",
   "metadata": {},
   "source": [
    "Now that I have fine tuned the Ridge Regression, let's see if there are other models that could perform better. The first one I want to try is Random Forest Regression. I will start with establishing a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "presidential-michigan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.9778821466705006 , Random Forest 2 MAE: 6956.267291585127\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_train, ', Random Forest 2 MAE:', rfr_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "binding-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.8525919728615938 , Random Forest 2 MAE: 18405.52422700587\n"
     ]
    }
   ],
   "source": [
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_test, ', Random Forest 2 MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-carrier",
   "metadata": {},
   "source": [
    "Looks like the Random Forest Regressor is overfitted and needs some refining. Let's start with hyperparameter tuning. I've been using this guide to identify which parameters to tune: https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/  \n",
    "I will build a loop to see which n_estimators provides the smallest train/test R2 score and MAE difference. After a preliminary run through, I have decided to start the iterations at n_estimators=50, going in 10 estimator increments up to 300 estimators (anything below 50 and above 300 yielded clearly poorer results). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stylish-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "r2_diff = []\n",
    "mae_diff = []\n",
    "\n",
    "for i in range(50, 310, 10):\n",
    "    rfr = RandomForestRegressor(n_estimators=i, oob_score=True, n_jobs=-1, random_state=123, \n",
    "                            max_features=\"auto\", min_samples_leaf=50)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    rfr_y_train_pred = rfr.predict(X_train)\n",
    "    rfr_r2_train = rfr.score(X_train, y_train)\n",
    "    rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "    #print({i}, 'TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)\n",
    "    rfr_y_test_pred = rfr.predict(X_test)\n",
    "    rfr_r2_test = rfr.score(X_test, y_test)\n",
    "    rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "    estimators.append(i)\n",
    "    r2_diff.append(rfr_r2_train - rfr_r2_test)\n",
    "    mae_diff.append(rfr_mae_test-rfr_mae_train) \n",
    "    #print({i}, 'TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "blessed-carroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04483304726304305 1046.5459769366753\n",
      "14 14\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "print(min(r2_diff), min(mae_diff))\n",
    "print(r2_diff.index(min(r2_diff)), mae_diff.index(min(mae_diff)))\n",
    "print(estimators[r2_diff.index(min(r2_diff))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-insert",
   "metadata": {},
   "source": [
    "Above, I have iterated over 50-300 n_estimators (in increments of 10) and found that the best result between train test score is given by n_estimators=190. Below I will rebuild the model with that hyper parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "frequent-notification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN R2: 0.6952763823835019 , MAE: 26615.647422751725\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=190, oob_score=True, n_jobs=-1, random_state=123, min_samples_leaf=50)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "undefined-intersection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.6504433351204588 , MAE: 27662.193399688404\n"
     ]
    }
   ],
   "source": [
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-conducting",
   "metadata": {},
   "source": [
    "Above, I have fine tuned the parameters for the Random Forest Regressor. We have a test R2 score of 0.6504 and MAE of 27662.1934. Not particularly great performance compared to my Ridge Regression model. Let's see if feature selection can improve the performance.  \n",
    "I will be using the method SalectFromModel (discussed in this article: https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f) to do my feature selection. First, I want to check out the feature importances from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "searching-testament",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.91696617e-04, 6.15152100e-03, 0.00000000e+00, 1.04074069e-02,\n",
       "       0.00000000e+00, 5.79236407e-05, 6.26694189e-02, 2.07832884e-02,\n",
       "       1.77886236e-03, 0.00000000e+00, 2.08626125e-01, 2.81567576e-05,\n",
       "       0.00000000e+00, 2.85556694e-02, 1.29848428e-03, 2.94683931e-04,\n",
       "       1.84578403e-05, 4.84919465e-04, 4.73323521e-01, 2.05015209e-02,\n",
       "       1.06138678e-04, 3.68739385e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.98746153e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.62199510e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.05447112e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.61503524e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.38171903e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.12613400e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.43764262e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.23381495e-03,\n",
       "       1.32749242e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       7.20715096e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       9.72976280e-03, 0.00000000e+00, 3.50094152e-03, 0.00000000e+00,\n",
       "       1.76796849e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.14731351e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 6.77861306e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.24291034e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.75400045e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.68101527e-04, 2.22151212e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.82956342e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.00016695e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.56135566e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.07249915e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.58181780e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.09569644e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-tracker",
   "metadata": {},
   "source": [
    "Looks like there are a lot of features that have 0 importance. I will begin with a feature selection of everything above 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "average-means",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "Index(['LotFrontage', 'LotArea', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF',\n",
      "       '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath',\n",
      "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n",
      "       'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'MSSubClass_30',\n",
      "       'MSSubClass_70', 'MSZoning_RM', 'LandContour_Bnk', 'HouseStyle_2.5Fin',\n",
      "       'OverallQual_6', 'YearRemodAdd_1951', 'ExterQual_TA', 'ExterCond_Ex',\n",
      "       'Foundation_Slab', 'BsmtQual_Fa', 'BsmtQual_Na', 'BsmtCond_Fa',\n",
      "       'BsmtFinType1_ALQ', 'BsmtFinType1_LwQ', 'HeatingQC_Fa', 'CentralAir_N',\n",
      "       'KitchenQual_TA', 'Functional_Maj1', 'FireplaceQu_Po',\n",
      "       'GarageType_Basment', 'GarageType_Na', 'GarageQual_Ex', 'GarageCond_Ex',\n",
      "       'PavedDrive_N'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sel = SelectFromModel(rfr, threshold=0.00001)\n",
    "sel.fit(X_train, y_train)\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "print(len(selected_feat))\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-polish",
   "metadata": {},
   "source": [
    "I will use the 43 features with the highest importance for this model printed above to create new X_train/X_test sets and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "meaning-triple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.6504433351204588 , MAE: 27662.193399688404\n"
     ]
    }
   ],
   "source": [
    "X_train_rfr = X_train[selected_feat]\n",
    "X_test_rfr = X_test[selected_feat]\n",
    "\n",
    "rfr.fit(X_train_rfr, y_train)\n",
    "rfr_y_test_pred = rfr.predict(X_test_rfr)\n",
    "rfr_r2_test = rfr.score(X_test_rfr, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-arthur",
   "metadata": {},
   "source": [
    "Selecting any subset of features (not printed above, just tried out) with the highest importance and dropping the rest led to a slightly worse performance for all subsets than when using all features. The best Random Forest Regression performance scores that I have found in this part of the project, are R2: 0.6504433351204588 , MAE: 27662.193399688396"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-phoenix",
   "metadata": {},
   "source": [
    "##### Light Gradient Boosted Machine Algorithm (LGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-buddy",
   "metadata": {},
   "source": [
    "So far, I have built a Ridge Regression model and a Random Forest model. The Ridge Regression model is currently the best performing one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-token",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
