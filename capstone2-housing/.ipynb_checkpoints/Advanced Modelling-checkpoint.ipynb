{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "million-tulsa",
   "metadata": {},
   "source": [
    "# Advanced Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-person",
   "metadata": {},
   "source": [
    "Now that I have found a base model to work with, let's see how I can improve upon or build a better competing model. In this part of the project, I will be attempting to optimize the Ridge Regression base model. In addition, I will build RandomForest and Light Gradient Boosting Machine (LGBM) models and compare their performance against the optimized Ridge Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "absent-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cooked-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../capstone2-housing/documents/final_housing_df.csv', index_col=0)\n",
    "X_train = pickle.load(open('X_train', 'rb'))\n",
    "X_test = pickle.load(open('X_test', 'rb'))\n",
    "y_train = pickle.load(open('y_train', 'rb'))\n",
    "y_test = pickle.load(open('y_test', 'rb'))\n",
    "\n",
    "model1 = pickle.load(open('RR_base', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-serbia",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-bristol",
   "metadata": {},
   "source": [
    "Okay, now that everything has been imported, the fun can begin. First, I want to try to improve on my Ridge Regression base model. In my base modelling, I identified the top three positive and top three negative importance features, let's retrace the steps and get a list of the top 10 coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-happiness",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "buried-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model1.coef_\n",
    "feature_dict = {}\n",
    "for coef, feat in zip(coefs, X_train.columns):\n",
    "    feature_dict[round(coef)] = feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "empty-bleeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145606, 124965, 92548, 72769, 66604, 62588, 51464, 47327, 43907, 43710]\n",
      "[-404345, -170360, -110046, -102164, -58605, -56044, -52028, -50921, -47994, -40665]\n"
     ]
    }
   ],
   "source": [
    "positive_coefs = sorted([round(coef) for coef in coefs if coef >=0], reverse=True)\n",
    "negative_coefs = sorted([round(coef) for coef in coefs if coef < 0], reverse=True, key=abs)\n",
    "top_pos_feat = positive_coefs[:10]\n",
    "top_neg_feat = negative_coefs[:10]\n",
    "\n",
    "print(top_pos_feat)\n",
    "print(top_neg_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "resident-strand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fence_GdPrv : 145606\n",
      "Exterior1st_AsbShng : 124965\n",
      "RoofMatl_Metal : 92548\n",
      "YearBuilt_1934 : 72769\n",
      "PoolQC_Fa : 66604\n",
      "RoofMatl_Membran : 62588\n",
      "RoofMatl_ClyTile : 51464\n",
      "OverallCond_1 : 47327\n",
      "RoofMatl_WdShngl : 43907\n",
      "RoofStyle_Flat : 43710\n",
      "RoofMatl_CompShg : -404345\n",
      "Condition2_RRAe : -170360\n",
      "PoolQC_Na : -110046\n",
      "PoolQC_Gd : -102164\n",
      "YearBuilt_1893 : -58605\n",
      "GarageYrBlt_1906.0 : -56044\n",
      "YearBuilt_1965 : -52028\n",
      "GarageYrBlt_1933.0 : -50921\n",
      "ExterCond_TA : -47994\n",
      "GarageYrBlt_1920.0 : -40665\n"
     ]
    }
   ],
   "source": [
    "top_features = positive_coefs[:10] + negative_coefs[:10]\n",
    "for i in top_features:\n",
    "    print(feature_dict.get(i), \":\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-reynolds",
   "metadata": {},
   "source": [
    "As there are many way we can approach this, I will try the following three subsets for feature selection: Top 10 Positive/10 Negative, Top 5 Positive/5 Negative, Top 3 Positive/3 Negative features. I will retrain the model for each of them and compare the results to see what difference it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-poetry",
   "metadata": {},
   "source": [
    "To do so, I will create a function that extracts the top k features based on the model coefficients, trains and evaluates the model with those features, and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equal-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_feature_score(k):\n",
    "    selected_features = []\n",
    "    top_k = positive_coefs[:k] + negative_coefs[:k]\n",
    "    for coef in top_k:\n",
    "        selected_features.append(feature_dict.get(coef))\n",
    "    X_train_k = X_train[selected_features]\n",
    "    X_test_k = X_test[selected_features]\n",
    "    model1.fit(X_train_k, y_train)\n",
    "    mod1_y_test_pred = model1.predict(X_test_k)\n",
    "    mod1_r2_test = model1.score(X_test_k, y_test)\n",
    "    mod1_mae_test = mean_absolute_error(y_test, mod1_y_test_pred)\n",
    "    return(mod1_r2_test, mod1_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-seminar",
   "metadata": {},
   "source": [
    "Time to find the best k value. I will use my function to iterate over k values the length of the list of negative coefficients (as that list is smaller than the list of positive coefficients). I will gather te results in two separate lists: R2 scores and MAE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "italic-subsection",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K with best R2 score: 190 , R2 Score: 0.8458229177969665 , MAE: 20716.53215373754\n",
      "K with smallest MAE: 200 , R2 Score: 0.8394433917505123 , MAE: 20691.08981478877\n"
     ]
    }
   ],
   "source": [
    "iterations = len(negative_coefs)\n",
    "r2s = []\n",
    "maes = []\n",
    "\n",
    "for num in range(1, iterations):\n",
    "    r2_score, mae_score = k_feature_score(num)\n",
    "    r2s.append(r2_score)\n",
    "    maes.append(mae_score)\n",
    "\n",
    "r2_index = r2s.index(max(r2s))\n",
    "mae_index = maes.index(min(maes))\n",
    "print('K with best R2 score:', r2_index+1, ', R2 Score:', max(r2s), ', MAE:', maes[r2_index])\n",
    "print('K with smallest MAE:', mae_index+1, ', R2 Score:', r2s[mae_index], ', MAE:', min(maes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-columbus",
   "metadata": {},
   "source": [
    "Based on the very low difference in MAE, the model performs best when for the top 190 features. I will train the model and summarize the scores below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "clear-portable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R2 score: 0.8458229177969665 , Ridge Regression MAE: 20716.53215373754\n"
     ]
    }
   ],
   "source": [
    "r2_score, mae_score = k_feature_score(190)\n",
    "print('Ridge Regression R2 score:', r2_score, ', Ridge Regression MAE:', mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-finland",
   "metadata": {},
   "source": [
    "##### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-minnesota",
   "metadata": {},
   "source": [
    "Now that I have fine tuned the Ridge Regression, let's see if there are other models that could perform better. The first one I want to try is Random Forest Regression. I will start with establishing a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "introductory-senior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.9778821466705006 , Random Forest 2 MAE: 6956.267291585127\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_train, ', Random Forest 2 MAE:', rfr_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "remarkable-dryer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.8525919728615938 , Random Forest 2 MAE: 18405.52422700587\n"
     ]
    }
   ],
   "source": [
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_test, ', Random Forest 2 MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-cooperative",
   "metadata": {},
   "source": [
    "Looks like the Random Forest Regressor is overfitted and needs some refining. Let's start with hyperparameter tuning. I've been using this guide to identify which parameters to tune: https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/  \n",
    "I will build a loop to see which n_estimators provides the smallest train/test R2 score and MAE difference. After a preliminary run through, I have decided to start the iterations at n_estimators=50, going in 10 estimator increments up to 300 estimators (anything below 50 and above 300 yielded clearly poorer results). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cathedral-copying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{50} TEST R2: 0.8549814645733403 , MAE: 18467.25210958904\n",
      "{60} TEST R2: 0.8558269976647231 , MAE: 18369.911187214613\n",
      "{70} TEST R2: 0.8566206110076304 , MAE: 18312.671506849318\n",
      "{80} TEST R2: 0.855871805812004 , MAE: 18351.67640410959\n",
      "{90} TEST R2: 0.8534162343685465 , MAE: 18407.209689062838\n",
      "{100} TEST R2: 0.8525919728615938 , MAE: 18405.52422700587\n",
      "{110} TEST R2: 0.8503965283754009 , MAE: 18478.167404376443\n",
      "{120} TEST R2: 0.8527321069656002 , MAE: 18355.790668623613\n",
      "{130} TEST R2: 0.8543758426100986 , MAE: 18254.03743489387\n",
      "{140} TEST R2: 0.854020987167779 , MAE: 18251.272471344702\n",
      "{150} TEST R2: 0.8530815367691508 , MAE: 18303.59262622309\n",
      "{160} TEST R2: 0.8523882443819647 , MAE: 18305.535689823875\n",
      "{170} TEST R2: 0.8525508598281965 , MAE: 18265.950673420055\n",
      "{180} TEST R2: 0.8525823462323945 , MAE: 18315.47362687541\n",
      "{190} TEST R2: 0.8525362941622763 , MAE: 18322.809521062933\n",
      "{200} TEST R2: 0.852599778140666 , MAE: 18354.16245596869\n",
      "{210} TEST R2: 0.8518568099339394 , MAE: 18363.67205852204\n",
      "{220} TEST R2: 0.8510257758752331 , MAE: 18389.021535314\n",
      "{230} TEST R2: 0.8507328700476173 , MAE: 18407.86927082447\n",
      "{240} TEST R2: 0.850539197156617 , MAE: 18429.92681833007\n",
      "{250} TEST R2: 0.8502948811611858 , MAE: 18452.047783953036\n",
      "{260} TEST R2: 0.8504725559569307 , MAE: 18417.04526117718\n",
      "{270} TEST R2: 0.849973512485741 , MAE: 18428.145807059504\n",
      "{280} TEST R2: 0.8501155514527466 , MAE: 18387.042840369024\n",
      "{290} TEST R2: 0.8500323243149579 , MAE: 18400.078642283555\n",
      "{300} TEST R2: 0.8498104366546408 , MAE: 18376.598057403782\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "r2_diff = []\n",
    "mae_diff = []\n",
    "\n",
    "for i in range(50, 310, 10):\n",
    "    rfr = RandomForestRegressor(n_estimators=i, oob_score=True, n_jobs=-1, random_state=123)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    rfr_y_train_pred = rfr.predict(X_train)\n",
    "    rfr_r2_train = rfr.score(X_train, y_train)\n",
    "    rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "    #print({i}, 'TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)\n",
    "    rfr_y_test_pred = rfr.predict(X_test)\n",
    "    rfr_r2_test = rfr.score(X_test, y_test)\n",
    "    rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "    estimators.append(i)\n",
    "    r2_diff.append(rfr_r2_train - rfr_r2_test)\n",
    "    mae_diff.append(rfr_mae_test-rfr_mae_train) \n",
    "    print({i}, 'TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "opened-treatment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12073705906933141 11256.259947814746\n",
      "2 2\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(min(r2_diff), min(mae_diff))\n",
    "print(r2_diff.index(min(r2_diff)), mae_diff.index(min(mae_diff)))\n",
    "print(estimators[r2_diff.index(min(r2_diff))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-saver",
   "metadata": {},
   "source": [
    "Above, I have iterated over 50-300 n_estimators (in increments of 10) and found that the best result between train test score is given by n_estimators=70. Below I will rebuild the model with that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "numeric-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN R2: 0.9773576700769618 , MAE: 7056.411559034573\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, random_state=123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "based-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.8566206110076304 , MAE: 18312.671506849318\n"
     ]
    }
   ],
   "source": [
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-anniversary",
   "metadata": {},
   "source": [
    "The model still appears overfitted, would it help to add a min_samples_leaf argument?  \n",
    "To find the best min_samples_leaf value, I will do the same iteration as above - this time iterating over leaf values 5 through 60 at increments of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "binding-growing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5} TEST R2: 0.8325684112450087 , MAE: 18991.70562192042\n",
      "{10} TEST R2: 0.8144302996547779 , MAE: 20028.647163941594\n",
      "{15} TEST R2: 0.7915916884157694 , MAE: 20915.483482605265\n",
      "{20} TEST R2: 0.7859444658831446 , MAE: 21289.720817489295\n",
      "{25} TEST R2: 0.7791961432734009 , MAE: 21982.218220841012\n",
      "{30} TEST R2: 0.7593840324796688 , MAE: 22928.173430763185\n",
      "{35} TEST R2: 0.7440867936380864 , MAE: 23775.1702358942\n",
      "{40} TEST R2: 0.7232216924814665 , MAE: 25011.702586415664\n",
      "{45} TEST R2: 0.6628129884756686 , MAE: 27024.72955344304\n",
      "{50} TEST R2: 0.6438298770284498 , MAE: 27928.9107132763\n",
      "{55} TEST R2: 0.6343858868236527 , MAE: 28309.927586214213\n",
      "{60} TEST R2: 0.6256813540648787 , MAE: 28783.477292694988\n"
     ]
    }
   ],
   "source": [
    "leaves = []\n",
    "r2_diff = []\n",
    "mae_diff = []\n",
    "\n",
    "for i in range(5, 65, 5):\n",
    "    rfr = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, random_state=123, min_samples_leaf=i)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    rfr_y_train_pred = rfr.predict(X_train)\n",
    "    rfr_r2_train = rfr.score(X_train, y_train)\n",
    "    rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "    #print({i}, 'TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)\n",
    "    rfr_y_test_pred = rfr.predict(X_test)\n",
    "    rfr_r2_test = rfr.score(X_test, y_test)\n",
    "    rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "    leaves.append(i)\n",
    "    r2_diff.append(rfr_r2_train - rfr_r2_test)\n",
    "    mae_diff.append(rfr_mae_test-rfr_mae_train) \n",
    "    print({i}, 'TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mobile-beatles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033337536897174225 939.78566070546\n",
      "5 11\n",
      "30 60\n"
     ]
    }
   ],
   "source": [
    "print(min(r2_diff), min(mae_diff))\n",
    "print(r2_diff.index(min(r2_diff)), mae_diff.index(min(mae_diff)))\n",
    "print(leaves[r2_diff.index(min(r2_diff))], leaves[mae_diff.index(min(mae_diff))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-smooth",
   "metadata": {},
   "source": [
    "After having tried both leaf values, I have found that choosing a min_samples_leaf=30 is the best choice as it reduces the overfitting without lowering the performance too much (using 60 significantly lowers both train and test performance). I will rebuild the random forest regressor below with n_estimators=70 and min_samples_leaf=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stable-pride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN R2: 0.792721569376843 , MAE: 21876.880455825136\n",
      "TEST R2: 0.7593840324796687 , MAE: 22928.173430763185\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, random_state=123, min_samples_leaf=30)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)\n",
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-melissa",
   "metadata": {},
   "source": [
    "Above, I have fine tuned the parameters for the Random Forest Regressor. We have a test R2 score of 0.7594 and MAE of 22928.1734. Not a great performance compared to my Ridge Regression model. Would feature selection be able to improve the performance even more?  \n",
    "For feature selection, I will be using the method SelectFromModel (with the parameters discussed in this article: https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f) to do my feature selection.  \n",
    "  \n",
    "First, I want to check out the feature importances from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "obvious-knowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.14464785e-03, 7.52869939e-03, 4.19805745e-03, 2.41211833e-02,\n",
       "       0.00000000e+00, 2.88929691e-04, 5.79135539e-02, 3.25553627e-02,\n",
       "       1.39117400e-03, 0.00000000e+00, 2.17015120e-01, 1.32015925e-04,\n",
       "       0.00000000e+00, 2.92720654e-02, 1.18602954e-03, 2.21635672e-04,\n",
       "       4.04398106e-05, 7.81140791e-04, 4.20417755e-01, 2.22589364e-02,\n",
       "       7.84814655e-05, 1.32218414e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.98065112e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.28444007e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 8.01771973e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.42545787e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.90682320e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.32523760e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.45483042e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.46256418e-05,\n",
       "       7.85585518e-05, 3.10040086e-03, 8.81900572e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.23381948e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.45619961e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.39838286e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.41844702e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.21730739e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.42026959e-03,\n",
       "       9.96056011e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.58858995e-04,\n",
       "       9.57673100e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.85193716e-02, 0.00000000e+00, 3.28374893e-03, 0.00000000e+00,\n",
       "       1.05055958e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.55320966e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.45991324e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.30620046e-05,\n",
       "       1.67604340e-03, 1.42864761e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.09975161e-03,\n",
       "       0.00000000e+00, 3.18297557e-04, 1.76695789e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 5.96507651e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.95244154e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.41186505e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.02756257e-03, 0.00000000e+00, 3.05691001e-05,\n",
       "       9.04296956e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 6.62099516e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       7.42325513e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.99892009e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-appliance",
   "metadata": {},
   "source": [
    "Looks like there are a lot of features that have 0 importance. I started with a feature selection of everything above 0, and after playing around with the different values, I found that using a feature importance threshold 0.0001 slightly improves the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "computational-footage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "Index(['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF',\n",
      "       'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath',\n",
      "       'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
      "       'GarageCars', 'WoodDeckSF', 'MSSubClass_40', 'MSSubClass_70',\n",
      "       'MSZoning_RM', 'BldgType_2fmCon', 'OverallQual_8', 'OverallQual_9',\n",
      "       'YearRemodAdd_1951', 'Exterior1st_Wd Sdng', 'ExterQual_TA',\n",
      "       'ExterCond_Ex', 'Foundation_PConc', 'Foundation_Slab', 'BsmtQual_Fa',\n",
      "       'BsmtQual_Na', 'BsmtCond_Fa', 'BsmtFinType1_LwQ', 'HeatingQC_Fa',\n",
      "       'CentralAir_Y', 'Electrical_FuseA', 'KitchenQual_Fa', 'KitchenQual_TA',\n",
      "       'Functional_Maj1', 'FireplaceQu_Po', 'GarageType_Basment',\n",
      "       'GarageType_Na', 'GarageFinish_Na', 'GarageQual_Ex', 'GarageCond_Ex',\n",
      "       'PavedDrive_N'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sel = SelectFromModel(rfr, threshold=0.0001)\n",
    "sel.fit(X_train, y_train)\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "print(len(selected_feat))\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-startup",
   "metadata": {},
   "source": [
    "I will use the 46 features with the highest importance for this model printed above to create new X_train/X_test sets and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "nuclear-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.759389711644032 , MAE: 22925.267378791483\n"
     ]
    }
   ],
   "source": [
    "X_train_rfr = X_train[selected_feat]\n",
    "X_test_rfr = X_test[selected_feat]\n",
    "\n",
    "rfr.fit(X_train_rfr, y_train)\n",
    "rfr_y_test_pred = rfr.predict(X_test_rfr)\n",
    "rfr_r2_test = rfr.score(X_test_rfr, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-slovakia",
   "metadata": {},
   "source": [
    "Selecting a subset of the 46 features with the highest importance slightly improved the performance of the model. The best Random Forest Regression performance scores that I have found in this part of the project are:  \n",
    "R2: 0.7594 and MAE: 22925.2674"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-pillow",
   "metadata": {},
   "source": [
    "##### Light Gradient Boosted Machine Algorithm (LGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-stuart",
   "metadata": {},
   "source": [
    "So far, I have built a Ridge Regression model and a Random Forest model. The Ridge Regression model is currently the best performing one.  \n",
    "Finally, I want to build a LGBM model to see if that model could outperform my Ridge Regression. As before, I will begin with building a base model, training and testing it on the data as is. For this part of the project, I have been using this site as a guide: https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "seasonal-catering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN R2: 0.9776385877338646 , MAE: 5647.614989434855\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMRegressor(random_state=123)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "print('TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "urban-narrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.8507357166979355 , MAE: 17327.200122753264\n"
     ]
    }
   ],
   "source": [
    "lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-montgomery",
   "metadata": {},
   "source": [
    "The train and test results show that the model is overfitted. Let's see if that can be fixed by hyperparameter tuning. I will begin with num_leaves. As before, I will iterate over a range of possible values of num_leaves (in incfrements of ten) and find which one performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "palestinian-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10} TEST R2: 0.8594634117211949 , MAE: 17397.849294308904\n",
      "{20} TEST R2: 0.8452620300251685 , MAE: 17968.286819192905\n",
      "{30} TEST R2: 0.8496770829011758 , MAE: 17519.3971804949\n",
      "{40} TEST R2: 0.8467046425125285 , MAE: 17855.905693430646\n",
      "{50} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{60} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{70} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{80} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{90} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{100} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{110} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{120} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{130} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{140} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n",
      "{150} TEST R2: 0.8495531433952419 , MAE: 17769.13229547067\n"
     ]
    }
   ],
   "source": [
    "leaves = []\n",
    "r2_diff = []\n",
    "mae_diff = []\n",
    "\n",
    "for i in range(10, 160, 10):\n",
    "    lgbm = LGBMRegressor(num_leaves=i, random_state=123)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "    lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "    lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "    #print({i}, 'TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)\n",
    "    lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "    lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "    lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "    leaves.append(i)\n",
    "    r2_diff.append(lgbm_r2_train - lgbm_r2_test)\n",
    "    mae_diff.append(lgbm_mae_test-lgbm_mae_train) \n",
    "    print({i}, 'TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "tribal-tribune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0987101633526759 6738.829073672934\n",
      "0 0\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "print(min(r2_diff), min(mae_diff))\n",
    "print(r2_diff.index(min(r2_diff)), mae_diff.index(min(mae_diff)))\n",
    "print(leaves[r2_diff.index(min(r2_diff))], leaves[mae_diff.index(min(mae_diff))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-superior",
   "metadata": {},
   "source": [
    "Setting the parameter num_leaves=10 clearly improved the model's testing performance, but it the model still is overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rotary-associate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN R2: 0.9581735750738708 , MAE: 10659.02022063597\n",
      "TEST R2: 0.8594634117211949 , MAE: 17397.849294308904\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMRegressor(num_leaves=10, random_state=123)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "print('TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)\n",
    "lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-mouse",
   "metadata": {},
   "source": [
    "To prevent overfitting, I will adjust the parameter min_data_in_leaf. As before, I will iterate over a range of possible values to see which leads to the better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "signed-munich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "{10} TEST R2: 0.8637098980850493 , MAE: 17067.585589870814\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "{20} TEST R2: 0.8594634117211949 , MAE: 17397.849294308904\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "{30} TEST R2: 0.8677917704363621 , MAE: 16973.465692653655\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=40\n",
      "{40} TEST R2: 0.86842942410872 , MAE: 16984.84373594008\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "{50} TEST R2: 0.857732491976405 , MAE: 17529.681041150707\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n",
      "{60} TEST R2: 0.8593371637312683 , MAE: 17489.426370786805\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=70, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=70\n",
      "{70} TEST R2: 0.8279364968618854 , MAE: 19184.18063181552\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=80, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=80\n",
      "{80} TEST R2: 0.8270402813748383 , MAE: 19381.032084283986\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=90, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=90\n",
      "{90} TEST R2: 0.8234612135755379 , MAE: 19506.0434165535\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "{100} TEST R2: 0.8092841717084351 , MAE: 20904.976086681156\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=110, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=110\n",
      "{110} TEST R2: 0.8096671495050073 , MAE: 21029.3749748495\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=120, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=120\n",
      "{120} TEST R2: 0.8063022346983 , MAE: 21078.79289465227\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=130, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=130\n",
      "{130} TEST R2: 0.7967275309675871 , MAE: 21964.26421474057\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=140, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=140\n",
      "{140} TEST R2: 0.8011568866276068 , MAE: 21570.993163111918\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "{150} TEST R2: 0.7945727459829284 , MAE: 22286.252829702436\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=160, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=160\n",
      "{160} TEST R2: 0.7943672786516713 , MAE: 22065.99464795908\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=170, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=170\n",
      "{170} TEST R2: 0.7844504682408888 , MAE: 22910.52586491235\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=180, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=180\n",
      "{180} TEST R2: 0.7828265889105003 , MAE: 23231.323712539393\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=190, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=190\n",
      "{190} TEST R2: 0.7849803132119955 , MAE: 23033.3455637588\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=200, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=200\n",
      "{200} TEST R2: 0.7869168298787891 , MAE: 22438.60058768341\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=210, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=210\n",
      "{210} TEST R2: 0.7766032142236025 , MAE: 23196.382408620222\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=220, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=220\n",
      "{220} TEST R2: 0.7706141230645001 , MAE: 23479.199308265448\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=230, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=230\n",
      "{230} TEST R2: 0.7623206399489837 , MAE: 23905.544650951764\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=240, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=240\n",
      "{240} TEST R2: 0.7620244939808156 , MAE: 24172.712401041288\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "{250} TEST R2: 0.7608022979787358 , MAE: 24203.460462735915\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=260, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=260\n",
      "{260} TEST R2: 0.752468429877722 , MAE: 24492.7086791277\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=270, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=270\n",
      "{270} TEST R2: 0.7442756207039241 , MAE: 24974.419704639386\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=280, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=280\n",
      "{280} TEST R2: 0.7249901544868917 , MAE: 26162.30886924831\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=290, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=290\n",
      "{290} TEST R2: 0.7198706217521604 , MAE: 26420.413673265637\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=300, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=300\n",
      "{300} TEST R2: 0.7157404493018251 , MAE: 26609.535541524023\n"
     ]
    }
   ],
   "source": [
    "leaves = []\n",
    "r2_diff = []\n",
    "mae_diff = []\n",
    "\n",
    "for i in range(10, 310, 10):\n",
    "    lgbm = LGBMRegressor(num_leaves=10, min_data_in_leaf=i, random_state=123)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "    lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "    lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "    #print({i}, 'TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)\n",
    "    lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "    lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "    lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "    leaves.append(i)\n",
    "    r2_diff.append(lgbm_r2_train - lgbm_r2_test)\n",
    "    mae_diff.append(lgbm_mae_test-lgbm_mae_train) \n",
    "    print({i}, 'TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "solved-watershed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02195973018842101 294.10834436078585\n",
      "24 24\n",
      "250 250\n"
     ]
    }
   ],
   "source": [
    "print(min(r2_diff), min(mae_diff))\n",
    "print(r2_diff.index(min(r2_diff)), mae_diff.index(min(mae_diff)))\n",
    "print(leaves[r2_diff.index(min(r2_diff))], leaves[mae_diff.index(min(mae_diff))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-receipt",
   "metadata": {},
   "source": [
    "There we go! Using num_leaves=10 and min_data_in_leaf=250 has minimized the overfitting. Let's rebuild the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sustained-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "TRAIN R2: 0.7827620281671568 , MAE: 23909.35211837513\n",
      "TEST R2: 0.7608022979787358 , MAE: 24203.460462735915\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMRegressor(num_leaves=10, min_data_in_leaf=250, random_state=123)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "print('TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)\n",
    "lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-police",
   "metadata": {},
   "source": [
    "Now that I have tuned some of the parameters, I will take a look at feature importance. I will recreate the steps I did for feature importance for my random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "nearby-project",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10,  4, 18,  0,  2, 15, 23,  6,  0, 39,  0,  0,  0, 11,  1,  1,\n",
       "        6,  2, 14,  7,  6,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  5,  0,\n",
       "        0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0,  0,  6,  0,  5,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  4,  0,  0,  4,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  4, 11,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  4,  0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  0,  0,  3,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "unavailable-chain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "34\n",
      "TEST R2: 0.7608022979787358 , MAE: 24203.460462735915\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "29\n",
      "TEST R2: 0.7598343010604406 , MAE: 24269.586699173396\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "27\n",
      "TEST R2: 0.757702660996993 , MAE: 24289.849569335798\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "25\n",
      "TEST R2: 0.7580642262802978 , MAE: 24216.48203657827\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "19\n",
      "TEST R2: 0.7515782269865307 , MAE: 24954.85677322027\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "15\n",
      "TEST R2: 0.7554922148259646 , MAE: 24153.064266330988\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "10\n",
      "TEST R2: 0.7381453209515632 , MAE: 25842.76462971077\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "9\n",
      "TEST R2: 0.7361814909457668 , MAE: 25921.848021195085\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "9\n",
      "TEST R2: 0.7361814909457668 , MAE: 25921.848021195085\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "9\n",
      "TEST R2: 0.7361814909457668 , MAE: 25921.848021195085\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "7\n",
      "TEST R2: 0.7187527459528948 , MAE: 26499.22168796868\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "5\n",
      "TEST R2: 0.6575851870010971 , MAE: 29045.906747069384\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "5\n",
      "TEST R2: 0.6575851870010971 , MAE: 29045.906747069384\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "5\n",
      "TEST R2: 0.6575851870010971 , MAE: 29045.906747069384\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "4\n",
      "TEST R2: 0.6104984217330727 , MAE: 31295.800308435006\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "3\n",
      "TEST R2: 0.562139710465301 , MAE: 33641.74239256328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "3\n",
      "TEST R2: 0.562139710465301 , MAE: 33641.74239256328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "3\n",
      "TEST R2: 0.562139710465301 , MAE: 33641.74239256328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "2\n",
      "TEST R2: 0.5529338508635426 , MAE: 33977.44204600175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "2\n",
      "TEST R2: 0.5529338508635426 , MAE: 33977.44204600175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "2\n",
      "TEST R2: 0.5529338508635426 , MAE: 33977.44204600175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "2\n",
      "TEST R2: 0.5529338508635426 , MAE: 33977.44204600175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "2\n",
      "TEST R2: 0.5529338508635426 , MAE: 33977.44204600175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=250, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=250\n",
      "1\n",
      "TEST R2: 0.4451225736201333 , MAE: 38364.691014902484\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 39):\n",
    "    lgbm = LGBMRegressor(num_leaves=10, min_data_in_leaf=250, random_state=123)\n",
    "    sel = SelectFromModel(lgbm, threshold=i)\n",
    "    sel.fit(X_train, y_train)\n",
    "    selected_feat= X_train.columns[(sel.get_support())]\n",
    "    print(len(selected_feat))  \n",
    "    X_train_lgbm = X_train[selected_feat]\n",
    "    X_test_lgbm = X_test[selected_feat]\n",
    "    lgbm.fit(X_train_lgbm, y_train)\n",
    "    lgbm_y_test_pred = lgbm.predict(X_test_lgbm)\n",
    "    lgbm_r2_test = lgbm.score(X_test_lgbm, y_test)\n",
    "    lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "    print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-sucking",
   "metadata": {},
   "source": [
    "Even for this model, choosing to keep all vs. dropping some columns seems to only make the model perform worse. My Ridge Regression is outperforming both the Random Forest and LGBM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-happening",
   "metadata": {},
   "source": [
    "##### Model Score Comparison table:  \n",
    "  \n",
    "\n",
    "| Model | R2 Score | MAE | Upper/Lower bound |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| Ridge Regression | 0.8458 | 20716.5322 | TBA |\n",
    "| Random Forest | 0.7594 | 22925.2674 | TBA |\n",
    "| LGBM | 0.7608 | 24203.4605 | TBA |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-silence",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
