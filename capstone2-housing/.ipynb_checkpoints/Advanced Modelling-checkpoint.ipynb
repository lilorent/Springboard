{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "julian-madonna",
   "metadata": {},
   "source": [
    "# Advanced Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-trade",
   "metadata": {},
   "source": [
    "Now that I have found a base model to work with, let's see how I can improve upon or build a better competing model. In this part of the project, I will be attempting to optimize the Ridge Regression base model. In addition, I will build RandomForest, Light Gradient Boosting Machine (LGBM), and Extreme Gradient Boosting (XGBoost) models and compare their performance against the optimized Ridge Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eight-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "iraqi-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../capstone2-housing/documents/final_housing_df.csv', index_col=0)\n",
    "X_train = pickle.load(open('X_train', 'rb'))\n",
    "X_test = pickle.load(open('X_test', 'rb'))\n",
    "y_train = pickle.load(open('y_train', 'rb'))\n",
    "y_test = pickle.load(open('y_test', 'rb'))\n",
    "\n",
    "model1 = pickle.load(open('RR_base', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-advance",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-coffee",
   "metadata": {},
   "source": [
    "Okay, now that everything has been imported, the fun can begin. First, I want to try to improve on my Ridge Regression base model. In my base modelling, I identified the top three positive and top three negative importance features, let's retrace the steps and get a list of the top 10 coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-bread",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "binary-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model1.coef_\n",
    "feature_dict = {}\n",
    "for coef, feat in zip(coefs, X_train.columns):\n",
    "    feature_dict[round(coef)] = feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "remarkable-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145606, 124965, 92548, 72769, 66604, 62588, 51464, 47327, 43907, 43710]\n",
      "[-404345, -170360, -110046, -102164, -58605, -56044, -52028, -50921, -47994, -40665]\n"
     ]
    }
   ],
   "source": [
    "positive_coefs = sorted([round(coef) for coef in coefs if coef >=0], reverse=True)\n",
    "negative_coefs = sorted([round(coef) for coef in coefs if coef < 0], reverse=True, key=abs)\n",
    "top_pos_feat = positive_coefs[:10]\n",
    "top_neg_feat = negative_coefs[:10]\n",
    "\n",
    "print(top_pos_feat)\n",
    "print(top_neg_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "close-globe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fence_GdPrv : 145606\n",
      "Exterior1st_AsbShng : 124965\n",
      "RoofMatl_Metal : 92548\n",
      "YearBuilt_1934 : 72769\n",
      "PoolQC_Fa : 66604\n",
      "RoofMatl_Membran : 62588\n",
      "RoofMatl_ClyTile : 51464\n",
      "OverallCond_1 : 47327\n",
      "RoofMatl_WdShngl : 43907\n",
      "RoofStyle_Flat : 43710\n",
      "RoofMatl_CompShg : -404345\n",
      "Condition2_RRAe : -170360\n",
      "PoolQC_Na : -110046\n",
      "PoolQC_Gd : -102164\n",
      "YearBuilt_1893 : -58605\n",
      "GarageYrBlt_1906.0 : -56044\n",
      "YearBuilt_1965 : -52028\n",
      "GarageYrBlt_1933.0 : -50921\n",
      "ExterCond_TA : -47994\n",
      "GarageYrBlt_1920.0 : -40665\n"
     ]
    }
   ],
   "source": [
    "top_features = positive_coefs[:10] + negative_coefs[:10]\n",
    "for i in top_features:\n",
    "    print(feature_dict.get(i), \":\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-northeast",
   "metadata": {},
   "source": [
    "As there are many way we can approach this, I will try the following three subsets for feature selection: Top 10 Positive/10 Negative, Top 5 Positive/5 Negative, Top 3 Positive/3 Negative features. I will retrain the model for each of them and compare the results to see what difference it makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-orleans",
   "metadata": {},
   "source": [
    "To do so, I will create a function that extracts the top k features based on the model coefficients, trains and evaluates the model with those features, and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "apparent-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_feature_score(k):\n",
    "    selected_features = []\n",
    "    top_k = positive_coefs[:k] + negative_coefs[:k]\n",
    "    for coef in top_k:\n",
    "        selected_features.append(feature_dict.get(coef))\n",
    "    X_train_k = X_train[selected_features]\n",
    "    X_test_k = X_test[selected_features]\n",
    "    model1.fit(X_train_k, y_train)\n",
    "    mod1_y_test_pred = model1.predict(X_test_k)\n",
    "    mod1_r2_test = model1.score(X_test_k, y_test)\n",
    "    mod1_mae_test = mean_absolute_error(y_test, mod1_y_test_pred)\n",
    "    return(mod1_r2_test, mod1_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-incident",
   "metadata": {},
   "source": [
    "Time to find the best k value. I will use my function to iterate over k values the length of the list of negative coefficients (as that list is smaller than the list of positive coefficients). I will gather te results in two separate lists: R2 scores and MAE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coastal-range",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K with best R2 score: 190 , R2 Score: 0.8458229177969665 , MAE: 20716.53215373754\n",
      "K with smallest MAE: 200 , R2 Score: 0.8394433917505123 , MAE: 20691.08981478877\n"
     ]
    }
   ],
   "source": [
    "iterations = len(negative_coefs)\n",
    "r2s = []\n",
    "maes = []\n",
    "\n",
    "for num in range(1, iterations):\n",
    "    r2_score, mae_score = k_feature_score(num)\n",
    "    r2s.append(r2_score)\n",
    "    maes.append(mae_score)\n",
    "\n",
    "r2_index = r2s.index(max(r2s))\n",
    "mae_index = maes.index(min(maes))\n",
    "print('K with best R2 score:', r2_index+1, ', R2 Score:', max(r2s), ', MAE:', maes[r2_index])\n",
    "print('K with smallest MAE:', mae_index+1, ', R2 Score:', r2s[mae_index], ', MAE:', min(maes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-texas",
   "metadata": {},
   "source": [
    "Based on the very low difference in MAE, the model performs best when for the top 190 features. I will train the model and summarize the scores below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rural-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression R2 score: 0.8458229177969665 , Ridge Regression MAE: 20716.53215373754\n"
     ]
    }
   ],
   "source": [
    "r2_score, mae_score = k_feature_score(190)\n",
    "print('Ridge Regression R2 score:', r2_score, ', Ridge Regression MAE:', mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-right",
   "metadata": {},
   "source": [
    "##### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-experiment",
   "metadata": {},
   "source": [
    "Now that I have fine tuned the Ridge Regression, let's see if there are other models that could perform better. The first one I want to try is Random Forest Regression. I will start with establishing a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sharing-dialogue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.9778821466705006 , Random Forest 2 MAE: 6956.267291585127\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_train, ', Random Forest 2 MAE:', rfr_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "changing-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.8525919728615938 , Random Forest 2 MAE: 18405.52422700587\n"
     ]
    }
   ],
   "source": [
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_test, ', Random Forest 2 MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-processor",
   "metadata": {},
   "source": [
    "Looks like the Random Forest Regressor is overfitted and needs some refining. Let's start with hyperparameter tuning. I've been using this guide to identify which parameters to tune: https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/  \n",
    "I will build a loop to see which n_estimators provides the smallest train/test R2 score and MAE difference. After a preliminary run through, I have decided to start the iterations at n_estimators=50, going in 10 estimator increments up to 300 estimators (anything below 50 and above 300 yielded clearly poorer results). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nervous-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "r2_diff = []\n",
    "mae_diff = []\n",
    "\n",
    "for i in range(50, 310, 10):\n",
    "    rfr = RandomForestRegressor(n_estimators=i, oob_score=True, n_jobs=-1, random_state=123)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    rfr_y_train_pred = rfr.predict(X_train)\n",
    "    rfr_r2_train = rfr.score(X_train, y_train)\n",
    "    rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "    #print({i}, 'TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)\n",
    "    rfr_y_test_pred = rfr.predict(X_test)\n",
    "    rfr_r2_test = rfr.score(X_test, y_test)\n",
    "    rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "    estimators.append(i)\n",
    "    r2_diff.append(rfr_r2_train - rfr_r2_test)\n",
    "    mae_diff.append(rfr_mae_test-rfr_mae_train) \n",
    "    #print({i}, 'TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "polished-tattoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12073705906933141 11256.259947814746\n",
      "2 2\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "print(min(r2_diff), min(mae_diff))\n",
    "print(r2_diff.index(min(r2_diff)), mae_diff.index(min(mae_diff)))\n",
    "print(estimators[r2_diff.index(min(r2_diff))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-ladder",
   "metadata": {},
   "source": [
    "Above, I have iterated over 50-300 n_estimators (in increments of 10) and found that the best result between train test score is given by n_estimators=70. Below I will rebuild the model with that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "valid-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN R2: 0.9773576700769618 , MAE: 7056.411559034573\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, random_state=123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "average-cradle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.8566206110076304 , MAE: 18312.671506849318\n"
     ]
    }
   ],
   "source": [
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-scientist",
   "metadata": {},
   "source": [
    "The model still appears overfitted, would it help to add a min_samples_leaf argument?  \n",
    "To find the best min_samples_leaf value, I will do the same iteration as above - this time iterating over leaf values 5 through 60 at increments of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "novel-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = []\n",
    "r2_diff = []\n",
    "mae_diff = []\n",
    "\n",
    "for i in range(5, 65, 5):\n",
    "    rfr = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, random_state=123, min_samples_leaf=i)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    rfr_y_train_pred = rfr.predict(X_train)\n",
    "    rfr_r2_train = rfr.score(X_train, y_train)\n",
    "    rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "    #print({i}, 'TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)\n",
    "    rfr_y_test_pred = rfr.predict(X_test)\n",
    "    rfr_r2_test = rfr.score(X_test, y_test)\n",
    "    rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "    leaves.append(i)\n",
    "    r2_diff.append(rfr_r2_train - rfr_r2_test)\n",
    "    mae_diff.append(rfr_mae_test-rfr_mae_train) \n",
    "    #print({i}, 'TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "headed-conference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033337536897174336 939.78566070546\n",
      "5 11\n",
      "30 60\n"
     ]
    }
   ],
   "source": [
    "print(min(r2_diff), min(mae_diff))\n",
    "print(r2_diff.index(min(r2_diff)), mae_diff.index(min(mae_diff)))\n",
    "print(leaves[r2_diff.index(min(r2_diff))], leaves[mae_diff.index(min(mae_diff))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-equality",
   "metadata": {},
   "source": [
    "After having tried both leaf values, I have found that choosing a min_samples_leaf=30 is the best choice as it reduces the overfitting without lowering the performance too much (using 60 significantly lowers both train and test performance). I will rebuild the random forest regressor below with n_estimators=70 and min_samples_leaf=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "buried-adult",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN R2: 0.792721569376843 , MAE: 21876.880455825136\n",
      "TEST R2: 0.7593840324796687 , MAE: 22928.173430763185\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=70, oob_score=True, n_jobs=-1, random_state=123, min_samples_leaf=30)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('TRAIN R2:', rfr_r2_train, ', MAE:', rfr_mae_train)\n",
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-vancouver",
   "metadata": {},
   "source": [
    "Above, I have fine tuned the parameters for the Random Forest Regressor. We have a test R2 score of 0.7594 and MAE of 22928.1734. Not a great performance compared to my Ridge Regression model. Would feature selection be able to improve the performance even more?  \n",
    "For feature selection, I will be using the method SelectFromModel (with the parameters discussed in this article: https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f) to do my feature selection.  \n",
    "  \n",
    "First, I want to check out the feature importances from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "chief-elimination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.14464785e-03, 7.52869939e-03, 4.19805745e-03, 2.41211833e-02,\n",
       "       0.00000000e+00, 2.88929691e-04, 5.79135539e-02, 3.25553627e-02,\n",
       "       1.39117400e-03, 0.00000000e+00, 2.17015120e-01, 1.32015925e-04,\n",
       "       0.00000000e+00, 2.92720654e-02, 1.18602954e-03, 2.21635672e-04,\n",
       "       4.04398106e-05, 7.81140791e-04, 4.20417755e-01, 2.22589364e-02,\n",
       "       7.84814655e-05, 1.32218414e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.98065112e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.28444007e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 8.01771973e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.42545787e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.90682320e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 9.32523760e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.45483042e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.46256418e-05,\n",
       "       7.85585518e-05, 3.10040086e-03, 8.81900572e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.23381948e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.45619961e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.39838286e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.41844702e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.21730739e-05,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.42026959e-03,\n",
       "       9.96056011e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.58858995e-04,\n",
       "       9.57673100e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.85193716e-02, 0.00000000e+00, 3.28374893e-03, 0.00000000e+00,\n",
       "       1.05055958e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 8.55320966e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.45991324e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.30620046e-05,\n",
       "       1.67604340e-03, 1.42864761e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.09975161e-03,\n",
       "       0.00000000e+00, 3.18297557e-04, 1.76695789e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 5.96507651e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.95244154e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.41186505e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.02756257e-03, 0.00000000e+00, 3.05691001e-05,\n",
       "       9.04296956e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 6.62099516e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       7.42325513e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.99892009e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-aruba",
   "metadata": {},
   "source": [
    "Looks like there are a lot of features that have 0 importance. I started with a feature selection of everything above 0, and after playing around with the different values, I found that using a feature importance threshold 0.0001 slightly improves the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "usual-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "Index(['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtUnfSF',\n",
      "       'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath',\n",
      "       'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
      "       'GarageCars', 'WoodDeckSF', 'MSSubClass_40', 'MSSubClass_70',\n",
      "       'MSZoning_RM', 'BldgType_2fmCon', 'OverallQual_8', 'OverallQual_9',\n",
      "       'YearRemodAdd_1951', 'Exterior1st_Wd Sdng', 'ExterQual_TA',\n",
      "       'ExterCond_Ex', 'Foundation_PConc', 'Foundation_Slab', 'BsmtQual_Fa',\n",
      "       'BsmtQual_Na', 'BsmtCond_Fa', 'BsmtFinType1_LwQ', 'HeatingQC_Fa',\n",
      "       'CentralAir_Y', 'Electrical_FuseA', 'KitchenQual_Fa', 'KitchenQual_TA',\n",
      "       'Functional_Maj1', 'FireplaceQu_Po', 'GarageType_Basment',\n",
      "       'GarageType_Na', 'GarageFinish_Na', 'GarageQual_Ex', 'GarageCond_Ex',\n",
      "       'PavedDrive_N'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sel = SelectFromModel(rfr, threshold=0.0001)\n",
    "sel.fit(X_train, y_train)\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "print(len(selected_feat))\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-capture",
   "metadata": {},
   "source": [
    "I will use the 46 features with the highest importance for this model printed above to create new X_train/X_test sets and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stable-jewelry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST R2: 0.759389711644032 , MAE: 22925.26737879148\n"
     ]
    }
   ],
   "source": [
    "X_train_rfr = X_train[selected_feat]\n",
    "X_test_rfr = X_test[selected_feat]\n",
    "\n",
    "rfr.fit(X_train_rfr, y_train)\n",
    "rfr_y_test_pred = rfr.predict(X_test_rfr)\n",
    "rfr_r2_test = rfr.score(X_test_rfr, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-local",
   "metadata": {},
   "source": [
    "Selecting a subset of the 46 features with the highest importance slightly improved the performance of the model. The best Random Forest Regression performance scores that I have found in this part of the project are:  \n",
    "R2: 0.7594 and MAE: 22925.2674"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-vampire",
   "metadata": {},
   "source": [
    "##### Light Gradient Boosted Machine Algorithm (LGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-forward",
   "metadata": {},
   "source": [
    "So far, I have built a Ridge Regression model and a Random Forest model. The Ridge Regression model is currently the best performing one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-kernel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-flexibility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "earlier-lighting",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
