{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metropolitan-arbitration",
   "metadata": {},
   "source": [
    "# Advanced Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-ethiopia",
   "metadata": {},
   "source": [
    "Now that I have found a base model to work with, let's see how I can improve upon or build a better competing model. In this part of the project, I will be attempting to optimize the Ridge Regression base model. In addition, I will build RandomForest and Light Gradient Boosting Machine (LGBM) models and compare their performance against the optimized Ridge Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hidden-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, learning_curve, KFold\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alone-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../capstone2-housing/documents/final_housing_df.csv', index_col=0)\n",
    "X_train = pickle.load(open('X_train', 'rb'))\n",
    "X_test = pickle.load(open('X_test', 'rb'))\n",
    "y_train = pickle.load(open('y_train', 'rb'))\n",
    "y_test = pickle.load(open('y_test', 'rb'))\n",
    "\n",
    "model1 = pickle.load(open('RR_base', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-pricing",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-sacrifice",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "democratic-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = model1.coef_\n",
    "feature_dict = {}\n",
    "for coef, feat in zip(coefs, X_train.columns):\n",
    "    feature_dict[round(coef)] = feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "skilled-wisconsin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145606, 124965, 92548, 72769, 66604, 62588, 51464, 47327, 43907, 43710]\n",
      "[-404345, -170360, -110046, -102164, -58605, -56044, -52028, -50921, -47994, -40665]\n"
     ]
    }
   ],
   "source": [
    "positive_coefs = sorted([round(coef) for coef in coefs if coef >=0], reverse=True)\n",
    "negative_coefs = sorted([round(coef) for coef in coefs if coef < 0], reverse=True, key=abs)\n",
    "top_pos_feat = positive_coefs[:10]\n",
    "top_neg_feat = negative_coefs[:10]\n",
    "\n",
    "print(top_pos_feat)\n",
    "print(top_neg_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "simplified-needle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fence_GdPrv : 145606\n",
      "Exterior1st_AsbShng : 124965\n",
      "RoofMatl_Metal : 92548\n",
      "YearBuilt_1934 : 72769\n",
      "PoolQC_Fa : 66604\n",
      "RoofMatl_Membran : 62588\n",
      "RoofMatl_ClyTile : 51464\n",
      "OverallCond_1 : 47327\n",
      "RoofMatl_WdShngl : 43907\n",
      "RoofStyle_Flat : 43710\n",
      "RoofMatl_CompShg : -404345\n",
      "Condition2_RRAe : -170360\n",
      "PoolQC_Na : -110046\n",
      "PoolQC_Gd : -102164\n",
      "YearBuilt_1893 : -58605\n",
      "GarageYrBlt_1906.0 : -56044\n",
      "YearBuilt_1965 : -52028\n",
      "GarageYrBlt_1933.0 : -50921\n",
      "ExterCond_TA : -47994\n",
      "GarageYrBlt_1920.0 : -40665\n"
     ]
    }
   ],
   "source": [
    "top_features = positive_coefs[:10] + negative_coefs[:10]\n",
    "for i in top_features:\n",
    "    print(feature_dict.get(i), \":\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-ballot",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-madrid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "designed-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score(clf, x, y, score_func=mean_absolute_error):\n",
    "    #print(str(type(x)) + ' ' + str(x.shape) + ' ' + str(type(y)) + ' ' + str(y.shape))\n",
    "    result_default = 0\n",
    "    result_mape = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        y_pred = clf.predict(x[test])\n",
    "        result_default += score_func(y_pred, y[test]) # evaluate score function on held-out data\n",
    "        result_mape += np.mean(np.abs((y[test] - y_pred)/y[test]))\n",
    "    return (result_default / nfold), (result_mape / nfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "present-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a number as an input, creates a mask from two global lists (positive_coefs, negative_coefs), \n",
    "# creates a new X_train with that mask, performs cross-validation, and returns a tuple \n",
    "\n",
    "def k_feature_score(k):\n",
    "    selected_features = []\n",
    "    top_k = positive_coefs[:k] + negative_coefs[:k]\n",
    "    for coef in top_k:\n",
    "        selected_features.append(feature_dict.get(coef))\n",
    "    X_train_k = X_train[selected_features]\n",
    "    X_train_k = X_train_k.to_numpy()\n",
    "    y_train_k = y_train.to_numpy()\n",
    "    mae_scores, mape_scores = cv_score(model1, X_train_k, y_train_k)\n",
    "    return (mae_scores, mape_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-minority",
   "metadata": {},
   "source": [
    "Time to find the best k value. Below, I will use my k_feature_score function to iterate over k values the length of the list of negative coefficients (as that list is smaller than the list of positive coefficients). I will gather the results in a dictionary with k-value as key and the MAE, MAPE tuple as value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "naughty-finder",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = len(negative_coefs)\n",
    "k_value_scores = []\n",
    "\n",
    "for num in range(1, iterations):\n",
    "    cv_scores = k_feature_score(num)\n",
    "    k_value_scores.append(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "driven-lesson",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-77b2418a343b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k_values' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-advisory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-music",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-settle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-parcel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-oliver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-dynamics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-submission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-cisco",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-symposium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "colonial-volleyball",
   "metadata": {},
   "source": [
    "##### Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-language",
   "metadata": {},
   "source": [
    "Now that I have fine tuned the Ridge Regression, let's see if there are other models that could perform better. The first one I want to try is Random Forest Regression. I will start with establishing a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "classical-scenario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.9778821466705006 , Random Forest 2 MAE: 6956.267291585127\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(random_state=123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_train, ', Random Forest 2 MAE:', rfr_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "heated-vertical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R2 score: 0.8525919728615938 , Random Forest 2 MAE: 18405.52422700587\n"
     ]
    }
   ],
   "source": [
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_test, ', Random Forest 2 MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-holocaust",
   "metadata": {},
   "source": [
    "Looks like the Random Forest Regressor is overfitted and needs some refining. Let's start with hyperparameter tuning. I've been using this guide to decide on which parameters to tune: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html#for-better-accuracy  \n",
    "\n",
    "I will do a grid search cross validation to see which n_estimators and min_leaf_sample score the lowest MAE. After a preliminary run through, I have decided to start the iterations at n_estimators=50, going in 10 estimator increments up to 140 estimators (anything below 50 and above 150 yielded clearly poorer results). For min_samples_leaf, I have decided to start the iterations at 5, going in 5 samples increments up to 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-least",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(oob_score=True, n_jobs=-1, random_state=123)\n",
    "params = {'n_estimators':range(50, 150, 10), 'min_samples_leaf':range(5, 65, 10)}\n",
    "grid_model = GridSearchCV(rfr, param_grid=params, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_model.fit(X_train, y_train)\n",
    "grid_model.best_params_, grid_model.best_score_, grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-belly",
   "metadata": {},
   "source": [
    "Looks lilke using a combination of min_samples_leaf=5 and n_estimators=140 results in the lowest MAE. Let's build the model below and see how it performs in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor(n_estimators=140, min_samples_leaf=5, oob_score=True, n_jobs=-1, random_state=123)\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_y_train_pred = rfr.predict(X_train)\n",
    "rfr_r2_train = rfr.score(X_train, y_train)\n",
    "rfr_mae_train = mean_absolute_error(y_train, rfr_y_train_pred)\n",
    "print('RFR R2 score:', rfr_r2_train, ', RFR MAE:', rfr_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_y_test_pred = rfr.predict(X_test)\n",
    "rfr_r2_test = rfr.score(X_test, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "print('Random Forest R2 score:', rfr_r2_test, ', Random Forest 2 MAE:', rfr_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-administration",
   "metadata": {},
   "source": [
    "Now that we have completed hyperparameter tuning. Let's do some feature selection. First, let's take a look at the feature importance for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-breath",
   "metadata": {},
   "source": [
    "Looks like there are a lot of features that have 0 importance. I started with a feature selection of everything above 0, and after playing around with the different values, I found that using a feature importance threshold 0.0000001 slightly improves the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectFromModel(rfr, threshold=0.0000001)\n",
    "sel.fit(X_train, y_train)\n",
    "selected_feat= X_train.columns[(sel.get_support())]\n",
    "print(len(selected_feat))\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-future",
   "metadata": {},
   "source": [
    "I will use the 46 features with the highest importance for this model printed above to create new X_train/X_test sets and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfr_mape_test calculates the mean average percentage error\n",
    "X_train_rfr = X_train[selected_feat]\n",
    "X_test_rfr = X_test[selected_feat]\n",
    "\n",
    "rfr.fit(X_train_rfr, y_train)\n",
    "rfr_y_test_pred = rfr.predict(X_test_rfr)\n",
    "rfr_r2_test = rfr.score(X_test_rfr, y_test)\n",
    "rfr_mae_test = mean_absolute_error(y_test, rfr_y_test_pred)\n",
    "rfr_mape_test = np.mean(np.abs((y_test - rfr_y_test_pred)/y_test)) * 100\n",
    "print('TEST R2:', rfr_r2_test, ', MAE:', rfr_mae_test, 'MAPE:', rfr_mape_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-essay",
   "metadata": {},
   "source": [
    "Selecting a subset of the 46 features with the highest importance slightly improved the performance of the model. The best Random Forest Regression performance scores that I have found in this part of the project are:  \n",
    "R2: 0.8269 and MAE: 19113.8267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-inquiry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-intersection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-domain",
   "metadata": {},
   "source": [
    "##### Light Gradient Boosted Machine Algorithm (LGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-charm",
   "metadata": {},
   "source": [
    "So far, I have built a Ridge Regression model and a Random Forest model. The Ridge Regression model is currently the best performing one.  \n",
    "Finally, I want to build a LGBM model to see if that model could outperform my Ridge Regression. As before, I will begin with building a base model, training and testing it on the data as is. For this part of the project, I have been using this site as a guide: https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor(random_state=123)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "print('TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-opening",
   "metadata": {},
   "source": [
    "The train and test results show that the model is overfitted. Let's see if that can be fixed by hyperparameter tuning. I will begin with num_leaves. As before, I will iterate over a range of possible values of num_leaves (in incfrements of ten) and find which one performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = []\n",
    "r2_diff = []\n",
    "mae_diff = []\n",
    "\n",
    "for i in range(10, 160, 10):\n",
    "    lgbm = LGBMRegressor(num_leaves=i, random_state=123)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "    lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "    lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "    #print({i}, 'TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)\n",
    "    lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "    lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "    lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "    leaves.append(i)\n",
    "    r2_diff.append(lgbm_r2_train - lgbm_r2_test)\n",
    "    mae_diff.append(lgbm_mae_test-lgbm_mae_train) \n",
    "    print({i}, 'TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(r2_diff), min(mae_diff))\n",
    "print(r2_diff.index(min(r2_diff)), mae_diff.index(min(mae_diff)))\n",
    "print(leaves[r2_diff.index(min(r2_diff))], leaves[mae_diff.index(min(mae_diff))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-classic",
   "metadata": {},
   "source": [
    "Setting the parameter num_leaves=10 clearly improved the model's testing performance, but it the model still is overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor(num_leaves=10, random_state=123)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "print('TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)\n",
    "lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-index",
   "metadata": {},
   "source": [
    "To prevent overfitting, I will adjust the parameter min_data_in_leaf. As before, I will iterate over a range of possible values to see which leads to the better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = []\n",
    "r2_diff = []\n",
    "mae_diff = []\n",
    "\n",
    "for i in range(10, 310, 10):\n",
    "    lgbm = LGBMRegressor(num_leaves=10, min_data_in_leaf=i, random_state=123)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "    lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "    lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "    #print({i}, 'TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)\n",
    "    lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "    lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "    lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "    leaves.append(i)\n",
    "    r2_diff.append(lgbm_r2_train - lgbm_r2_test)\n",
    "    mae_diff.append(lgbm_mae_test-lgbm_mae_train) \n",
    "    print({i}, 'TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-wholesale",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(min(r2_diff), min(mae_diff))\n",
    "print(r2_diff.index(min(r2_diff)), mae_diff.index(min(mae_diff)))\n",
    "print(leaves[r2_diff.index(min(r2_diff))], leaves[mae_diff.index(min(mae_diff))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-judges",
   "metadata": {},
   "source": [
    "There we go! Using num_leaves=10 and min_data_in_leaf=250 has minimized the overfitting. Let's rebuild the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMRegressor(num_leaves=10, min_data_in_leaf=250, random_state=123)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_y_train_pred = lgbm.predict(X_train)\n",
    "lgbm_r2_train = lgbm.score(X_train, y_train)\n",
    "lgbm_mae_train = mean_absolute_error(y_train, lgbm_y_train_pred)\n",
    "print('TRAIN R2:', lgbm_r2_train, ', MAE:', lgbm_mae_train)\n",
    "lgbm_y_test_pred = lgbm.predict(X_test)\n",
    "lgbm_r2_test = lgbm.score(X_test, y_test)\n",
    "lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-suspect",
   "metadata": {},
   "source": [
    "Now that I have tuned some of the parameters, I will take a look at feature importance. I will recreate the steps I did for feature importance for my random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 39):\n",
    "    lgbm = LGBMRegressor(num_leaves=10, min_data_in_leaf=250, random_state=123)\n",
    "    sel = SelectFromModel(lgbm, threshold=i)\n",
    "    sel.fit(X_train, y_train)\n",
    "    selected_feat= X_train.columns[(sel.get_support())]\n",
    "    print(len(selected_feat))  \n",
    "    X_train_lgbm = X_train[selected_feat]\n",
    "    X_test_lgbm = X_test[selected_feat]\n",
    "    lgbm.fit(X_train_lgbm, y_train)\n",
    "    lgbm_y_test_pred = lgbm.predict(X_test_lgbm)\n",
    "    lgbm_r2_test = lgbm.score(X_test_lgbm, y_test)\n",
    "    lgbm_mae_test = mean_absolute_error(y_test, lgbm_y_test_pred)\n",
    "    print('TEST R2:', lgbm_r2_test, ', MAE:', lgbm_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-supply",
   "metadata": {},
   "source": [
    "Even for this model, choosing to keep all vs. dropping some columns seems to only make the model perform worse. My Ridge Regression is outperforming both the Random Forest and LGBM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-timeline",
   "metadata": {},
   "source": [
    "##### Model Score Comparison table:  \n",
    "  \n",
    "\n",
    "| Model | R2 Score | MAE | Upper/Lower bound |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| Ridge Regression | 0.8458 | 20716.5322 | 126454.59/54560.03 |\n",
    "| Random Forest | 0.7594 | 22925.2674 | TBA |\n",
    "| LGBM | 0.7608 | 24203.4605 | TBA |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-heading",
   "metadata": {},
   "source": [
    "Looking at the above comparison, I can conclude that the Ridge Regression comparably performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-difficulty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rr_test_residuals = y_test - rfr_y_test_pred\n",
    "_ = plt.hist(rr_test_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_k = int(len(rr_test_residuals)*0.95 / 2)\n",
    "print(len(rr_test_residuals), half_k)\n",
    "sorted_pos = sorted([resid for resid in rr_test_residuals if resid >= 0])\n",
    "sorted_neg = sorted([abs(resid) for resid in rr_test_residuals if resid < 0])\n",
    "test_residuals_pos = sorted_pos[0:half_k]\n",
    "test_residuals_neg = sorted_neg[0:half_k]\n",
    "print(len(test_residuals_pos), len(test_residuals_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_lower = max(test_residuals_pos)\n",
    "worst_upper = max(test_residuals_neg)\n",
    "print(round(worst_lower, 2), round(worst_upper, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-amazon",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
